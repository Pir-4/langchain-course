{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2026-03-01T11:05:59.366276Z",
     "start_time": "2026-03-01T11:05:57.487470Z"
    }
   },
   "source": [
    "from numpy.f2py.crackfortran import verbose\n",
    "\n",
    "from chats import llm\n",
    "from langchain_classic.memory import ConversationBufferMemory\n",
    "\n",
    "memory = ConversationBufferMemory(return_messages=True)"
   ],
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/dc/v08dhs892w5ffjzk9_wrf6800000gn/T/ipykernel_38463/3347954859.py:4: LangChainDeprecationWarning: Please see the migration guide at: https://python.langchain.com/docs/versions/migrating_memory/\n",
      "  memory = ConversationBufferMemory(return_messages=True)\n"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-03-01T11:07:55.608775Z",
     "start_time": "2026-03-01T11:07:55.597332Z"
    }
   },
   "cell_type": "code",
   "source": [
    "memory.save_context(\n",
    "    {\"input\": \"Hi, my name is James\"},  # user message\n",
    "    {\"output\": \"Hey James, what's up? I'm an AI model called Zeta.\"}  # AI response\n",
    ")\n",
    "memory.save_context(\n",
    "    {\"input\": \"I'm researching the different types of conversational memory.\"},  # user message\n",
    "    {\"output\": \"That's interesting, what are some examples?\"}  # AI response\n",
    ")\n",
    "memory.save_context(\n",
    "    {\"input\": \"I've been looking at ConversationBufferMemory and ConversationBufferWindowMemory.\"},  # user message\n",
    "    {\"output\": \"That's interesting, what's the difference?\"}  # AI response\n",
    ")\n",
    "memory.save_context(\n",
    "    {\"input\": \"Buffer memory just stores the entire conversation, right?\"},  # user message\n",
    "    {\"output\": \"That makes sense, what about ConversationBufferWindowMemory?\"}  # AI response\n",
    ")\n",
    "memory.save_context(\n",
    "    {\"input\": \"Buffer window memory stores the last k messages, dropping the rest.\"},  # user message\n",
    "    {\"output\": \"Very cool!\"}  # AI response\n",
    ")"
   ],
   "id": "509c5b11fc7acdc5",
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-03-01T11:08:09.504751Z",
     "start_time": "2026-03-01T11:08:09.459536Z"
    }
   },
   "cell_type": "code",
   "source": "memory.load_memory_variables({})",
   "id": "f30b41ce4b372dda",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'history': [HumanMessage(content='Hi, my name is James', additional_kwargs={}, response_metadata={}),\n",
       "  AIMessage(content=\"Hey James, what's up? I'm an AI model called Zeta.\", additional_kwargs={}, response_metadata={}, tool_calls=[], invalid_tool_calls=[]),\n",
       "  HumanMessage(content=\"I'm researching the different types of conversational memory.\", additional_kwargs={}, response_metadata={}),\n",
       "  AIMessage(content=\"That's interesting, what are some examples?\", additional_kwargs={}, response_metadata={}, tool_calls=[], invalid_tool_calls=[]),\n",
       "  HumanMessage(content=\"I've been looking at ConversationBufferMemory and ConversationBufferWindowMemory.\", additional_kwargs={}, response_metadata={}),\n",
       "  AIMessage(content=\"That's interesting, what's the difference?\", additional_kwargs={}, response_metadata={}, tool_calls=[], invalid_tool_calls=[]),\n",
       "  HumanMessage(content='Buffer memory just stores the entire conversation, right?', additional_kwargs={}, response_metadata={}),\n",
       "  AIMessage(content='That makes sense, what about ConversationBufferWindowMemory?', additional_kwargs={}, response_metadata={}, tool_calls=[], invalid_tool_calls=[]),\n",
       "  HumanMessage(content='Buffer window memory stores the last k messages, dropping the rest.', additional_kwargs={}, response_metadata={}),\n",
       "  AIMessage(content='Very cool!', additional_kwargs={}, response_metadata={}, tool_calls=[], invalid_tool_calls=[])]}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-03-01T11:09:24.965553Z",
     "start_time": "2026-03-01T11:09:24.941664Z"
    }
   },
   "cell_type": "code",
   "source": [
    "memory = ConversationBufferMemory(return_messages=True)\n",
    "memory.chat_memory.add_user_message(\"Hi, my name is James\")\n",
    "memory.chat_memory.add_ai_message(\"Hey James, what's up? I'm an AI model called Zeta.\")\n",
    "memory.chat_memory.add_user_message(\"I'm researching the different types of conversational memory.\")\n",
    "memory.chat_memory.add_ai_message(\"That's interesting, what are some examples?\")\n",
    "memory.chat_memory.add_user_message(\"I've been looking at ConversationBufferMemory and ConversationBufferWindowMemory.\")\n",
    "memory.chat_memory.add_ai_message(\"That's interesting, what's the difference?\")\n",
    "memory.chat_memory.add_user_message(\"Buffer memory just stores the entire conversation, right?\")\n",
    "memory.chat_memory.add_ai_message(\"That makes sense, what about ConversationBufferWindowMemory?\")\n",
    "memory.chat_memory.add_user_message(\"Buffer window memory stores the last k messages, dropping the rest.\")\n",
    "memory.chat_memory.add_ai_message(\"Very cool!\")\n",
    "\n",
    "memory.load_memory_variables({})"
   ],
   "id": "536df59c5ce6ad61",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'history': [HumanMessage(content='Hi, my name is James', additional_kwargs={}, response_metadata={}),\n",
       "  AIMessage(content=\"Hey James, what's up? I'm an AI model called Zeta.\", additional_kwargs={}, response_metadata={}, tool_calls=[], invalid_tool_calls=[]),\n",
       "  HumanMessage(content=\"I'm researching the different types of conversational memory.\", additional_kwargs={}, response_metadata={}),\n",
       "  AIMessage(content=\"That's interesting, what are some examples?\", additional_kwargs={}, response_metadata={}, tool_calls=[], invalid_tool_calls=[]),\n",
       "  HumanMessage(content=\"I've been looking at ConversationBufferMemory and ConversationBufferWindowMemory.\", additional_kwargs={}, response_metadata={}),\n",
       "  AIMessage(content=\"That's interesting, what's the difference?\", additional_kwargs={}, response_metadata={}, tool_calls=[], invalid_tool_calls=[]),\n",
       "  HumanMessage(content='Buffer memory just stores the entire conversation, right?', additional_kwargs={}, response_metadata={}),\n",
       "  AIMessage(content='That makes sense, what about ConversationBufferWindowMemory?', additional_kwargs={}, response_metadata={}, tool_calls=[], invalid_tool_calls=[]),\n",
       "  HumanMessage(content='Buffer window memory stores the last k messages, dropping the rest.', additional_kwargs={}, response_metadata={}),\n",
       "  AIMessage(content='Very cool!', additional_kwargs={}, response_metadata={}, tool_calls=[], invalid_tool_calls=[])]}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-03-01T11:14:10.422097Z",
     "start_time": "2026-03-01T11:14:10.409547Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from  langchain_core.prompts import (\n",
    "SystemMessagePromptTemplate,\n",
    "HumanMessagePromptTemplate,\n",
    "MessagesPlaceholder,\n",
    "ChatPromptTemplate,\n",
    ")\n",
    "\n",
    "system_prompt = \"You are a helpful assistant called Zeta\"\n",
    "prompt_template = ChatPromptTemplate.from_messages([\n",
    "    SystemMessagePromptTemplate.from_template(system_prompt),\n",
    "    MessagesPlaceholder(variable_name=\"history\"),\n",
    "    HumanMessagePromptTemplate.from_template(\"{query}\")\n",
    "])"
   ],
   "id": "b3a168723655d324",
   "outputs": [],
   "execution_count": 7
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-03-01T11:15:07.010572Z",
     "start_time": "2026-03-01T11:15:06.996921Z"
    }
   },
   "cell_type": "code",
   "source": "pipeline = prompt_template | llm",
   "id": "4760a3dd4cdd5b08",
   "outputs": [],
   "execution_count": 8
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-03-01T11:17:44.593141Z",
     "start_time": "2026-03-01T11:17:44.572901Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from  langchain_core.chat_history import InMemoryChatMessageHistory\n",
    "\n",
    "chat_map = {}\n",
    "def get_that_history(session_id:str) -> InMemoryChatMessageHistory:\n",
    "    if session_id not in chat_map:\n",
    "        chat_map[session_id] = InMemoryChatMessageHistory()\n",
    "    return chat_map[session_id]"
   ],
   "id": "baf2fb9bed097e25",
   "outputs": [],
   "execution_count": 9
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-03-01T11:20:21.330658Z",
     "start_time": "2026-03-01T11:20:21.302275Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from  langchain_core.runnables.history import RunnableWithMessageHistory\n",
    "\n",
    "pipline_with_history = RunnableWithMessageHistory(\n",
    "    pipeline,\n",
    "    get_session_history=get_that_history,\n",
    "    input_messages_key=\"query\",\n",
    "    history_messages_key=\"history\"\n",
    ")"
   ],
   "id": "9560b3712b9a5a72",
   "outputs": [],
   "execution_count": 10
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-03-01T11:21:32.964951Z",
     "start_time": "2026-03-01T11:21:13.382048Z"
    }
   },
   "cell_type": "code",
   "source": [
    "pipline_with_history.invoke(\n",
    "    {\"query\": \"Hi, my name is James\"},\n",
    "    config={\"session_id\": \"id_123\"}\n",
    ")"
   ],
   "id": "c374633558a56449",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content=\"Hello, James! It's a pleasure to meet you. How are you doing today? I'd love to hear more about you or anything you'd like to discuss. ðŸ˜Š\", additional_kwargs={}, response_metadata={'model': 'qwen3:8b', 'created_at': '2026-03-01T11:21:32.839599Z', 'done': True, 'done_reason': 'stop', 'total_duration': 18835419459, 'load_duration': 4297030042, 'prompt_eval_count': 29, 'prompt_eval_duration': 1127660750, 'eval_count': 135, 'eval_duration': 13177751255, 'logprobs': None, 'model_name': 'qwen3:8b', 'model_provider': 'ollama'}, id='lc_run--019ca921-8c11-75d2-abfd-5f82c6498497-0', tool_calls=[], invalid_tool_calls=[], usage_metadata={'input_tokens': 29, 'output_tokens': 135, 'total_tokens': 164})"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 11
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-03-01T11:23:37.448811Z",
     "start_time": "2026-03-01T11:23:17.902413Z"
    }
   },
   "cell_type": "code",
   "source": [
    "pipline_with_history.invoke(\n",
    "    {\"query\": \"What is my name again?\"},\n",
    "    config={\"session_id\": \"id_123\"}\n",
    ")"
   ],
   "id": "b9b6b69bba016e7c",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='Your name is James! ðŸ˜Š Is there anything I can assist you with today?', additional_kwargs={}, response_metadata={'model': 'qwen3:8b', 'created_at': '2026-03-01T11:23:37.40612Z', 'done': True, 'done_reason': 'stop', 'total_duration': 19491325042, 'load_duration': 104398875, 'prompt_eval_count': 114, 'prompt_eval_duration': 2070703833, 'eval_count': 180, 'eval_duration': 17128174959, 'logprobs': None, 'model_name': 'qwen3:8b', 'model_provider': 'ollama'}, id='lc_run--019ca923-7019-73d1-b2d2-dc286a4129c0-0', tool_calls=[], invalid_tool_calls=[], usage_metadata={'input_tokens': 114, 'output_tokens': 180, 'total_tokens': 294})"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 14
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-03-01T11:23:49.927033Z",
     "start_time": "2026-03-01T11:23:49.876400Z"
    }
   },
   "cell_type": "code",
   "source": "print(chat_map)",
   "id": "6a680d820602e9e9",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'id_123': InMemoryChatMessageHistory(messages=[HumanMessage(content='Hi, my name is James', additional_kwargs={}, response_metadata={}), AIMessage(content=\"Hello, James! It's a pleasure to meet you. How are you doing today? I'd love to hear more about you or anything you'd like to discuss. ðŸ˜Š\", additional_kwargs={}, response_metadata={'model': 'qwen3:8b', 'created_at': '2026-03-01T11:21:32.839599Z', 'done': True, 'done_reason': 'stop', 'total_duration': 18835419459, 'load_duration': 4297030042, 'prompt_eval_count': 29, 'prompt_eval_duration': 1127660750, 'eval_count': 135, 'eval_duration': 13177751255, 'logprobs': None, 'model_name': 'qwen3:8b', 'model_provider': 'ollama'}, id='lc_run--019ca921-8c11-75d2-abfd-5f82c6498497-0', tool_calls=[], invalid_tool_calls=[], usage_metadata={'input_tokens': 29, 'output_tokens': 135, 'total_tokens': 164}), HumanMessage(content='What is my name again?', additional_kwargs={}, response_metadata={}), AIMessage(content='Your name is James! ðŸ˜Š Is there anything I can help you with today?', additional_kwargs={}, response_metadata={'model': 'qwen3:8b', 'created_at': '2026-03-01T11:23:10.985929Z', 'done': True, 'done_reason': 'stop', 'total_duration': 13353299500, 'load_duration': 121301042, 'prompt_eval_count': 81, 'prompt_eval_duration': 2138830709, 'eval_count': 109, 'eval_duration': 10963924746, 'logprobs': None, 'model_name': 'qwen3:8b', 'model_provider': 'ollama'}, id='lc_run--019ca923-20dd-78e2-ba28-7989dd3d9a47-0', tool_calls=[], invalid_tool_calls=[], usage_metadata={'input_tokens': 81, 'output_tokens': 109, 'total_tokens': 190}), HumanMessage(content='What is my name again?', additional_kwargs={}, response_metadata={}), AIMessage(content='Your name is James! ðŸ˜Š Is there anything I can assist you with today?', additional_kwargs={}, response_metadata={'model': 'qwen3:8b', 'created_at': '2026-03-01T11:23:37.40612Z', 'done': True, 'done_reason': 'stop', 'total_duration': 19491325042, 'load_duration': 104398875, 'prompt_eval_count': 114, 'prompt_eval_duration': 2070703833, 'eval_count': 180, 'eval_duration': 17128174959, 'logprobs': None, 'model_name': 'qwen3:8b', 'model_provider': 'ollama'}, id='lc_run--019ca923-7019-73d1-b2d2-dc286a4129c0-0', tool_calls=[], invalid_tool_calls=[], usage_metadata={'input_tokens': 114, 'output_tokens': 180, 'total_tokens': 294})])}\n"
     ]
    }
   ],
   "execution_count": 15
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-03-01T11:26:50.974654Z",
     "start_time": "2026-03-01T11:26:50.935481Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from langchain_classic.memory import ConversationBufferWindowMemory\n",
    "\n",
    "memory = ConversationBufferWindowMemory(k=4, return_messages=True)"
   ],
   "id": "7c2d314f62c3c67b",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/dc/v08dhs892w5ffjzk9_wrf6800000gn/T/ipykernel_38463/392160499.py:3: LangChainDeprecationWarning: Please see the migration guide at: https://python.langchain.com/docs/versions/migrating_memory/\n",
      "  memory = ConversationBufferWindowMemory(k=4, return_messages=True)\n"
     ]
    }
   ],
   "execution_count": 16
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-03-01T11:27:26.912431Z",
     "start_time": "2026-03-01T11:27:26.886298Z"
    }
   },
   "cell_type": "code",
   "source": [
    "memory.chat_memory.add_user_message(\"Hi, my name is James\")\n",
    "memory.chat_memory.add_ai_message(\"Hey James, what's up? I'm an AI model called Zeta.\")\n",
    "memory.chat_memory.add_user_message(\"I'm researching the different types of conversational memory.\")\n",
    "memory.chat_memory.add_ai_message(\"That's interesting, what are some examples?\")\n",
    "memory.chat_memory.add_user_message(\"I've been looking at ConversationBufferMemory and ConversationBufferWindowMemory.\")\n",
    "memory.chat_memory.add_ai_message(\"That's interesting, what's the difference?\")\n",
    "memory.chat_memory.add_user_message(\"Buffer memory just stores the entire conversation, right?\")\n",
    "memory.chat_memory.add_ai_message(\"That makes sense, what about ConversationBufferWindowMemory?\")\n",
    "memory.chat_memory.add_user_message(\"Buffer window memory stores the last k messages, dropping the rest.\")\n",
    "memory.chat_memory.add_ai_message(\"Very cool!\")\n",
    "\n",
    "memory.load_memory_variables({})"
   ],
   "id": "77f2a01902902270",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'history': [HumanMessage(content=\"I'm researching the different types of conversational memory.\", additional_kwargs={}, response_metadata={}),\n",
       "  AIMessage(content=\"That's interesting, what are some examples?\", additional_kwargs={}, response_metadata={}, tool_calls=[], invalid_tool_calls=[]),\n",
       "  HumanMessage(content=\"I've been looking at ConversationBufferMemory and ConversationBufferWindowMemory.\", additional_kwargs={}, response_metadata={}),\n",
       "  AIMessage(content=\"That's interesting, what's the difference?\", additional_kwargs={}, response_metadata={}, tool_calls=[], invalid_tool_calls=[]),\n",
       "  HumanMessage(content='Buffer memory just stores the entire conversation, right?', additional_kwargs={}, response_metadata={}),\n",
       "  AIMessage(content='That makes sense, what about ConversationBufferWindowMemory?', additional_kwargs={}, response_metadata={}, tool_calls=[], invalid_tool_calls=[]),\n",
       "  HumanMessage(content='Buffer window memory stores the last k messages, dropping the rest.', additional_kwargs={}, response_metadata={}),\n",
       "  AIMessage(content='Very cool!', additional_kwargs={}, response_metadata={}, tool_calls=[], invalid_tool_calls=[])]}"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 17
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-03-01T11:30:27.605096Z",
     "start_time": "2026-03-01T11:30:27.575646Z"
    }
   },
   "cell_type": "code",
   "source": "from langchain_classic.chains import ConversationChain",
   "id": "fe17ccfd635f1761",
   "outputs": [],
   "execution_count": 18
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-03-01T11:31:08.277291Z",
     "start_time": "2026-03-01T11:30:43.184219Z"
    }
   },
   "cell_type": "code",
   "source": [
    "chain = ConversationChain(\n",
    "    llm=llm,\n",
    "    memory=memory,\n",
    "    verbose=True\n",
    ")\n",
    "chain.invoke({\"input\": \"what is my name again?\"})"
   ],
   "id": "fd8691114f5201f",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/dc/v08dhs892w5ffjzk9_wrf6800000gn/T/ipykernel_38463/3752517664.py:1: LangChainDeprecationWarning: The class `ConversationChain` was deprecated in LangChain 0.2.7 and will be removed in 1.0. Use `langchain_core.runnables.history.RunnableWithMessageHistory` instead.\n",
      "  chain = ConversationChain(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001B[1m> Entering new ConversationChain chain...\u001B[0m\n",
      "Prompt after formatting:\n",
      "\u001B[32;1m\u001B[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
      "\n",
      "Current conversation:\n",
      "[HumanMessage(content=\"I'm researching the different types of conversational memory.\", additional_kwargs={}, response_metadata={}), AIMessage(content=\"That's interesting, what are some examples?\", additional_kwargs={}, response_metadata={}, tool_calls=[], invalid_tool_calls=[]), HumanMessage(content=\"I've been looking at ConversationBufferMemory and ConversationBufferWindowMemory.\", additional_kwargs={}, response_metadata={}), AIMessage(content=\"That's interesting, what's the difference?\", additional_kwargs={}, response_metadata={}, tool_calls=[], invalid_tool_calls=[]), HumanMessage(content='Buffer memory just stores the entire conversation, right?', additional_kwargs={}, response_metadata={}), AIMessage(content='That makes sense, what about ConversationBufferWindowMemory?', additional_kwargs={}, response_metadata={}, tool_calls=[], invalid_tool_calls=[]), HumanMessage(content='Buffer window memory stores the last k messages, dropping the rest.', additional_kwargs={}, response_metadata={}), AIMessage(content='Very cool!', additional_kwargs={}, response_metadata={}, tool_calls=[], invalid_tool_calls=[])]\n",
      "Human: what is my name again?\n",
      "AI:\u001B[0m\n",
      "\n",
      "\u001B[1m> Finished chain.\u001B[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'input': 'what is my name again?',\n",
       " 'history': [HumanMessage(content=\"I'm researching the different types of conversational memory.\", additional_kwargs={}, response_metadata={}),\n",
       "  AIMessage(content=\"That's interesting, what are some examples?\", additional_kwargs={}, response_metadata={}, tool_calls=[], invalid_tool_calls=[]),\n",
       "  HumanMessage(content=\"I've been looking at ConversationBufferMemory and ConversationBufferWindowMemory.\", additional_kwargs={}, response_metadata={}),\n",
       "  AIMessage(content=\"That's interesting, what's the difference?\", additional_kwargs={}, response_metadata={}, tool_calls=[], invalid_tool_calls=[]),\n",
       "  HumanMessage(content='Buffer memory just stores the entire conversation, right?', additional_kwargs={}, response_metadata={}),\n",
       "  AIMessage(content='That makes sense, what about ConversationBufferWindowMemory?', additional_kwargs={}, response_metadata={}, tool_calls=[], invalid_tool_calls=[]),\n",
       "  HumanMessage(content='Buffer window memory stores the last k messages, dropping the rest.', additional_kwargs={}, response_metadata={}),\n",
       "  AIMessage(content='Very cool!', additional_kwargs={}, response_metadata={}, tool_calls=[], invalid_tool_calls=[])],\n",
       " 'response': \"I don't have access to your name from our conversation history. Could you please share your name so I can remember it for future interactions?\"}"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 19
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-03-01T12:06:47.056780Z",
     "start_time": "2026-03-01T12:06:47.042256Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from  pydantic import  BaseModel, Field\n",
    "from  langchain_core.chat_history import BaseChatMessageHistory\n",
    "from  langchain_core.messages import BaseMessage\n",
    "\n",
    "class BufferWindowMessageHistory(BaseChatMessageHistory, BaseModel):\n",
    "    messages: list[BaseMessage] = Field(default_factory=list)\n",
    "    k: int = Field(default_factory=int)\n",
    "\n",
    "    def __init__(self, k: int):\n",
    "        super().__init__(k=k)\n",
    "        print(f\"Initializing BufferWindowMessageHistory with k={k}\")\n",
    "\n",
    "    def add_messages(self, messages: list[BaseMessage])->None:\n",
    "        self.messages.extend(messages)\n",
    "        self.messages = self.messages[-self.k:]\n",
    "\n",
    "    def clear(self) -> None:\n",
    "        self.messages = []\n"
   ],
   "id": "f6007b055eb61a10",
   "outputs": [],
   "execution_count": 39
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-03-01T12:06:49.028131Z",
     "start_time": "2026-03-01T12:06:49.013949Z"
    }
   },
   "cell_type": "code",
   "source": [
    "chat_map = {}\n",
    "def get_chat_history(session_id:str, k:int =4) -> BufferWindowMessageHistory:\n",
    "    print(f'get_chat_history with session_id={session_id} and k={k}')\n",
    "    if session_id not in chat_map:\n",
    "        chat_map[session_id] = BufferWindowMessageHistory(k=k)\n",
    "    return chat_map[session_id]"
   ],
   "id": "79d8c868b0f3e785",
   "outputs": [],
   "execution_count": 40
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-03-01T12:06:50.770345Z",
     "start_time": "2026-03-01T12:06:50.756751Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from langchain_core.runnables import ConfigurableFieldSpec\n",
    "\n",
    "pipeline_with_history = RunnableWithMessageHistory(\n",
    "    pipeline,\n",
    "    get_session_history=get_chat_history,\n",
    "    input_messages_key=\"query\",\n",
    "    history_messages_key=\"history\",\n",
    "    history_factory_config=[\n",
    "        ConfigurableFieldSpec(\n",
    "            id=\"session_id\",\n",
    "            annotation=str,\n",
    "            name=\"Session ID\",\n",
    "            description=\"The session ID to use for the chat history\",\n",
    "            default=\"id_default\",\n",
    "        ),\n",
    "        ConfigurableFieldSpec(\n",
    "            id=\"k\",\n",
    "            annotation=int,\n",
    "            name=\"k\",\n",
    "            description=\"The number of messages to keep in the history\",\n",
    "            default=4,\n",
    "        )\n",
    "    ]\n",
    ")"
   ],
   "id": "b3d1d9d6988c9c45",
   "outputs": [],
   "execution_count": 41
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-03-01T12:07:08.793538Z",
     "start_time": "2026-03-01T12:06:53.044380Z"
    }
   },
   "cell_type": "code",
   "source": [
    "pipeline_with_history.invoke(\n",
    "    {\"query\": \"Hi, my name is Valentin\"},\n",
    "    config={\"configurable\": {\"session_id\": \"id_k4\", \"k\":4}}\n",
    ")"
   ],
   "id": "5534e5e96fb9957a",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "get_chat_history with session_id=id_k4 and k=4\n",
      "Initializing BufferWindowMessageHistory with k=4\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "AIMessage(content=\"Hello, Valentin! It's a pleasure to meet you. How are you today? I hope you're having a great day! ðŸ˜Š\", additional_kwargs={}, response_metadata={'model': 'qwen3:8b', 'created_at': '2026-03-01T12:07:08.729386Z', 'done': True, 'done_reason': 'stop', 'total_duration': 15675394792, 'load_duration': 3422951083, 'prompt_eval_count': 30, 'prompt_eval_duration': 1180256833, 'eval_count': 122, 'eval_duration': 10898333912, 'logprobs': None, 'model_name': 'qwen3:8b', 'model_provider': 'ollama'}, id='lc_run--019ca94b-577c-7e90-9f21-9f8b60800b45-0', tool_calls=[], invalid_tool_calls=[], usage_metadata={'input_tokens': 30, 'output_tokens': 122, 'total_tokens': 152})"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 42
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-03-01T12:07:12.102065Z",
     "start_time": "2026-03-01T12:07:12.057371Z"
    }
   },
   "cell_type": "code",
   "source": [
    "chat_map[\"id_k4\"].clear()  # clear the history\n",
    "# manually insert history\n",
    "chat_map[\"id_k4\"].add_user_message(\"Hi, my name is Valentin\")\n",
    "chat_map[\"id_k4\"].add_ai_message(\"I'm an AI model called Zeta.\")\n",
    "chat_map[\"id_k4\"].add_user_message(\"I'm researching the different types of conversational memory.\")\n",
    "chat_map[\"id_k4\"].add_ai_message(\"That's interesting, what are some examples?\")\n",
    "chat_map[\"id_k4\"].add_user_message(\"I've been looking at ConversationBufferMemory and ConversationBufferWindowMemory.\")\n",
    "chat_map[\"id_k4\"].add_ai_message(\"That's interesting, what's the difference?\")\n",
    "chat_map[\"id_k4\"].add_user_message(\"Buffer memory just stores the entire conversation, right?\")\n",
    "chat_map[\"id_k4\"].add_ai_message(\"That makes sense, what about ConversationBufferWindowMemory?\")\n",
    "chat_map[\"id_k4\"].add_user_message(\"Buffer window memory stores the last k messages, dropping the rest.\")\n",
    "chat_map[\"id_k4\"].add_ai_message(\"Very cool!\")\n",
    "\n",
    "chat_map[\"id_k4\"].messages"
   ],
   "id": "2cc914df512dc2a5",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[HumanMessage(content='Buffer memory just stores the entire conversation, right?', additional_kwargs={}, response_metadata={}),\n",
       " AIMessage(content='That makes sense, what about ConversationBufferWindowMemory?', additional_kwargs={}, response_metadata={}, tool_calls=[], invalid_tool_calls=[]),\n",
       " HumanMessage(content='Buffer window memory stores the last k messages, dropping the rest.', additional_kwargs={}, response_metadata={}),\n",
       " AIMessage(content='Very cool!', additional_kwargs={}, response_metadata={}, tool_calls=[], invalid_tool_calls=[])]"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 43
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-03-01T12:07:42.480640Z",
     "start_time": "2026-03-01T12:07:15.095750Z"
    }
   },
   "cell_type": "code",
   "source": [
    "pipeline_with_history.invoke(\n",
    "    {\"query\": \"what is my name again?\"},\n",
    "    config={\"configurable\": {\"session_id\": \"id_k4\", \"k\": 4}}\n",
    ")"
   ],
   "id": "2c846e9ca9030a03",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "get_chat_history with session_id=id_k4 and k=4\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "AIMessage(content=\"I don't have that information stored. Could you please share your name with me?\", additional_kwargs={}, response_metadata={'model': 'qwen3:8b', 'created_at': '2026-03-01T12:07:42.430573Z', 'done': True, 'done_reason': 'stop', 'total_duration': 27320853583, 'load_duration': 113761916, 'prompt_eval_count': 86, 'prompt_eval_duration': 860009833, 'eval_count': 274, 'eval_duration': 26102600538, 'logprobs': None, 'model_name': 'qwen3:8b', 'model_provider': 'ollama'}, id='lc_run--019ca94b-ada4-77b2-89b7-baffd656caaa-0', tool_calls=[], invalid_tool_calls=[], usage_metadata={'input_tokens': 86, 'output_tokens': 274, 'total_tokens': 360})"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 44
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-03-01T12:12:41.571408Z",
     "start_time": "2026-03-01T12:12:18.534302Z"
    }
   },
   "cell_type": "code",
   "source": [
    "pipeline_with_history.invoke(\n",
    "    {\"query\": \"Hi, my name is Valentin\"},\n",
    "    config={\"configurable\": {\"session_id\": \"id_k14\", \"k\": 14}}\n",
    ")"
   ],
   "id": "d5ed37aeb396a84f",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "get_chat_history with session_id=id_k14 and k=14\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "AIMessage(content=\"Hi Valentin! ðŸ˜Š Nice to meet you. I'm Zeta, and I'm here to help with whatever you needâ€”whether it's about memory types, conversations, or anything else! What can I assist you with today?\", additional_kwargs={}, response_metadata={'model': 'qwen3:8b', 'created_at': '2026-03-01T12:12:41.486097Z', 'done': True, 'done_reason': 'stop', 'total_duration': 22935854333, 'load_duration': 117426458, 'prompt_eval_count': 252, 'prompt_eval_duration': 3568638083, 'eval_count': 213, 'eval_duration': 19026468423, 'logprobs': None, 'model_name': 'qwen3:8b', 'model_provider': 'ollama'}, id='lc_run--019ca950-4ef3-7612-9fbf-dd596e99994e-0', tool_calls=[], invalid_tool_calls=[], usage_metadata={'input_tokens': 252, 'output_tokens': 213, 'total_tokens': 465})"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 50
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-03-01T12:12:47.588107Z",
     "start_time": "2026-03-01T12:12:47.553498Z"
    }
   },
   "cell_type": "code",
   "source": [
    "chat_map[\"id_k14\"].add_user_message(\"I'm researching the different types of conversational memory.\")\n",
    "chat_map[\"id_k14\"].add_ai_message(\"That's interesting, what are some examples?\")\n",
    "chat_map[\"id_k14\"].add_user_message(\"I've been looking at ConversationBufferMemory and ConversationBufferWindowMemory.\")\n",
    "chat_map[\"id_k14\"].add_ai_message(\"That's interesting, what's the difference?\")\n",
    "chat_map[\"id_k14\"].add_user_message(\"Buffer memory just stores the entire conversation, right?\")\n",
    "chat_map[\"id_k14\"].add_ai_message(\"That makes sense, what about ConversationBufferWindowMemory?\")\n",
    "chat_map[\"id_k14\"].add_user_message(\"Buffer window memory stores the last k messages, dropping the rest.\")\n",
    "chat_map[\"id_k14\"].add_ai_message(\"Very cool!\")\n",
    "\n",
    "chat_map[\"id_k14\"].messages"
   ],
   "id": "49ac60537fd7cc05",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[HumanMessage(content='Buffer window memory stores the last k messages, dropping the rest.', additional_kwargs={}, response_metadata={}),\n",
       " AIMessage(content='Very cool!', additional_kwargs={}, response_metadata={}, tool_calls=[], invalid_tool_calls=[]),\n",
       " HumanMessage(content='what is my name again?', additional_kwargs={}, response_metadata={}),\n",
       " AIMessage(content=\"I don't have access to your name unless you've shared it in our conversation so far. Could you let me know your name? ðŸ˜Š\", additional_kwargs={}, response_metadata={'model': 'qwen3:8b', 'created_at': '2026-03-01T12:11:45.483435Z', 'done': True, 'done_reason': 'stop', 'total_duration': 36921682833, 'load_duration': 114755500, 'prompt_eval_count': 239, 'prompt_eval_duration': 3376151334, 'eval_count': 346, 'eval_duration': 33067379463, 'logprobs': None, 'model_name': 'qwen3:8b', 'model_provider': 'ollama'}, id='lc_run--019ca94f-3d8d-7d82-8faa-6c7c5ed63fd4-0', tool_calls=[], invalid_tool_calls=[], usage_metadata={'input_tokens': 239, 'output_tokens': 346, 'total_tokens': 585}),\n",
       " HumanMessage(content='Hi, my name is Valentin', additional_kwargs={}, response_metadata={}),\n",
       " AIMessage(content=\"Hi Valentin! ðŸ˜Š Nice to meet you. I'm Zeta, and I'm here to help with whatever you needâ€”whether it's about memory types, conversations, or anything else! What can I assist you with today?\", additional_kwargs={}, response_metadata={'model': 'qwen3:8b', 'created_at': '2026-03-01T12:12:41.486097Z', 'done': True, 'done_reason': 'stop', 'total_duration': 22935854333, 'load_duration': 117426458, 'prompt_eval_count': 252, 'prompt_eval_duration': 3568638083, 'eval_count': 213, 'eval_duration': 19026468423, 'logprobs': None, 'model_name': 'qwen3:8b', 'model_provider': 'ollama'}, id='lc_run--019ca950-4ef3-7612-9fbf-dd596e99994e-0', tool_calls=[], invalid_tool_calls=[], usage_metadata={'input_tokens': 252, 'output_tokens': 213, 'total_tokens': 465}),\n",
       " HumanMessage(content=\"I'm researching the different types of conversational memory.\", additional_kwargs={}, response_metadata={}),\n",
       " AIMessage(content=\"That's interesting, what are some examples?\", additional_kwargs={}, response_metadata={}, tool_calls=[], invalid_tool_calls=[]),\n",
       " HumanMessage(content=\"I've been looking at ConversationBufferMemory and ConversationBufferWindowMemory.\", additional_kwargs={}, response_metadata={}),\n",
       " AIMessage(content=\"That's interesting, what's the difference?\", additional_kwargs={}, response_metadata={}, tool_calls=[], invalid_tool_calls=[]),\n",
       " HumanMessage(content='Buffer memory just stores the entire conversation, right?', additional_kwargs={}, response_metadata={}),\n",
       " AIMessage(content='That makes sense, what about ConversationBufferWindowMemory?', additional_kwargs={}, response_metadata={}, tool_calls=[], invalid_tool_calls=[]),\n",
       " HumanMessage(content='Buffer window memory stores the last k messages, dropping the rest.', additional_kwargs={}, response_metadata={}),\n",
       " AIMessage(content='Very cool!', additional_kwargs={}, response_metadata={}, tool_calls=[], invalid_tool_calls=[])]"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 51
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-03-01T12:13:37.962330Z",
     "start_time": "2026-03-01T12:13:12.813057Z"
    }
   },
   "cell_type": "code",
   "source": [
    "pipeline_with_history.invoke(\n",
    "    {\"query\": \"what is my name again?\"},\n",
    "    config={\"session_id\": \"id_k14\", \"k\": 14}\n",
    ")"
   ],
   "id": "dfab89bf05d517a8",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "get_chat_history with session_id=id_k14 and k=14\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "AIMessage(content='Your name is Valentin! ðŸ˜Š I remember now. Nice to meet you again! How can I assist you with your research on conversational memory today?', additional_kwargs={}, response_metadata={'model': 'qwen3:8b', 'created_at': '2026-03-01T12:13:37.861444Z', 'done': True, 'done_reason': 'stop', 'total_duration': 25036697209, 'load_duration': 107304042, 'prompt_eval_count': 285, 'prompt_eval_duration': 2636168375, 'eval_count': 234, 'eval_duration': 22049076584, 'logprobs': None, 'model_name': 'qwen3:8b', 'model_provider': 'ollama'}, id='lc_run--019ca951-22f5-7f92-9df8-b1740b6c32a8-0', tool_calls=[], invalid_tool_calls=[], usage_metadata={'input_tokens': 285, 'output_tokens': 234, 'total_tokens': 519})"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 52
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-03-01T12:15:06.566493Z",
     "start_time": "2026-03-01T12:15:06.530746Z"
    }
   },
   "cell_type": "code",
   "source": "chat_map[\"id_k14\"].messages",
   "id": "45bd94c464959c50",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[HumanMessage(content='what is my name again?', additional_kwargs={}, response_metadata={}),\n",
       " AIMessage(content=\"I don't have access to your name unless you've shared it in our conversation so far. Could you let me know your name? ðŸ˜Š\", additional_kwargs={}, response_metadata={'model': 'qwen3:8b', 'created_at': '2026-03-01T12:11:45.483435Z', 'done': True, 'done_reason': 'stop', 'total_duration': 36921682833, 'load_duration': 114755500, 'prompt_eval_count': 239, 'prompt_eval_duration': 3376151334, 'eval_count': 346, 'eval_duration': 33067379463, 'logprobs': None, 'model_name': 'qwen3:8b', 'model_provider': 'ollama'}, id='lc_run--019ca94f-3d8d-7d82-8faa-6c7c5ed63fd4-0', tool_calls=[], invalid_tool_calls=[], usage_metadata={'input_tokens': 239, 'output_tokens': 346, 'total_tokens': 585}),\n",
       " HumanMessage(content='Hi, my name is Valentin', additional_kwargs={}, response_metadata={}),\n",
       " AIMessage(content=\"Hi Valentin! ðŸ˜Š Nice to meet you. I'm Zeta, and I'm here to help with whatever you needâ€”whether it's about memory types, conversations, or anything else! What can I assist you with today?\", additional_kwargs={}, response_metadata={'model': 'qwen3:8b', 'created_at': '2026-03-01T12:12:41.486097Z', 'done': True, 'done_reason': 'stop', 'total_duration': 22935854333, 'load_duration': 117426458, 'prompt_eval_count': 252, 'prompt_eval_duration': 3568638083, 'eval_count': 213, 'eval_duration': 19026468423, 'logprobs': None, 'model_name': 'qwen3:8b', 'model_provider': 'ollama'}, id='lc_run--019ca950-4ef3-7612-9fbf-dd596e99994e-0', tool_calls=[], invalid_tool_calls=[], usage_metadata={'input_tokens': 252, 'output_tokens': 213, 'total_tokens': 465}),\n",
       " HumanMessage(content=\"I'm researching the different types of conversational memory.\", additional_kwargs={}, response_metadata={}),\n",
       " AIMessage(content=\"That's interesting, what are some examples?\", additional_kwargs={}, response_metadata={}, tool_calls=[], invalid_tool_calls=[]),\n",
       " HumanMessage(content=\"I've been looking at ConversationBufferMemory and ConversationBufferWindowMemory.\", additional_kwargs={}, response_metadata={}),\n",
       " AIMessage(content=\"That's interesting, what's the difference?\", additional_kwargs={}, response_metadata={}, tool_calls=[], invalid_tool_calls=[]),\n",
       " HumanMessage(content='Buffer memory just stores the entire conversation, right?', additional_kwargs={}, response_metadata={}),\n",
       " AIMessage(content='That makes sense, what about ConversationBufferWindowMemory?', additional_kwargs={}, response_metadata={}, tool_calls=[], invalid_tool_calls=[]),\n",
       " HumanMessage(content='Buffer window memory stores the last k messages, dropping the rest.', additional_kwargs={}, response_metadata={}),\n",
       " AIMessage(content='Very cool!', additional_kwargs={}, response_metadata={}, tool_calls=[], invalid_tool_calls=[]),\n",
       " HumanMessage(content='what is my name again?', additional_kwargs={}, response_metadata={}),\n",
       " AIMessage(content='Your name is Valentin! ðŸ˜Š I remember now. Nice to meet you again! How can I assist you with your research on conversational memory today?', additional_kwargs={}, response_metadata={'model': 'qwen3:8b', 'created_at': '2026-03-01T12:13:37.861444Z', 'done': True, 'done_reason': 'stop', 'total_duration': 25036697209, 'load_duration': 107304042, 'prompt_eval_count': 285, 'prompt_eval_duration': 2636168375, 'eval_count': 234, 'eval_duration': 22049076584, 'logprobs': None, 'model_name': 'qwen3:8b', 'model_provider': 'ollama'}, id='lc_run--019ca951-22f5-7f92-9df8-b1740b6c32a8-0', tool_calls=[], invalid_tool_calls=[], usage_metadata={'input_tokens': 285, 'output_tokens': 234, 'total_tokens': 519})]"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 53
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-03-01T12:16:20.854516Z",
     "start_time": "2026-03-01T12:16:20.818122Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from langchain_classic.memory import ConversationSummaryMemory\n",
    "\n",
    "memory = ConversationSummaryMemory(llm=llm)"
   ],
   "id": "792acbf64fff3410",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/dc/v08dhs892w5ffjzk9_wrf6800000gn/T/ipykernel_38463/3945359647.py:3: LangChainDeprecationWarning: Please see the migration guide at: https://python.langchain.com/docs/versions/migrating_memory/\n",
      "  memory = ConversationSummaryMemory(llm=llm)\n"
     ]
    }
   ],
   "execution_count": 55
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-03-01T12:28:11.971518Z",
     "start_time": "2026-03-01T12:17:17.764460Z"
    }
   },
   "cell_type": "code",
   "source": [
    "chain = ConversationChain(\n",
    "    llm=llm,\n",
    "    memory = memory,\n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "chain.invoke({\"input\": \"hello there my name is James\"})\n",
    "chain.invoke({\"input\": \"I am researching the different types of conversational memory.\"})\n",
    "chain.invoke({\"input\": \"I have been looking at ConversationBufferMemory and ConversationBufferWindowMemory.\"})\n",
    "chain.invoke({\"input\": \"Buffer memory just stores the entire conversation\"})\n",
    "chain.invoke({\"input\": \"Buffer window memory stores the last k messages, dropping the rest.\"})"
   ],
   "id": "2cc21e0b507d6314",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001B[1m> Entering new ConversationChain chain...\u001B[0m\n",
      "Prompt after formatting:\n",
      "\u001B[32;1m\u001B[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
      "\n",
      "Current conversation:\n",
      "\n",
      "Human: hello there my name is James\n",
      "AI:\u001B[0m\n",
      "\n",
      "\u001B[1m> Finished chain.\u001B[0m\n",
      "\n",
      "\n",
      "\u001B[1m> Entering new ConversationChain chain...\u001B[0m\n",
      "Prompt after formatting:\n",
      "\u001B[32;1m\u001B[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
      "\n",
      "Current conversation:\n",
      "New summary: James greets the AI and introduces himself. The AI responds warmly, welcoming him and offering assistance with various tasks.\n",
      "Human: I am researching the different types of conversational memory.\n",
      "AI:\u001B[0m\n",
      "\n",
      "\u001B[1m> Finished chain.\u001B[0m\n",
      "\n",
      "\n",
      "\u001B[1m> Entering new ConversationChain chain...\u001B[0m\n",
      "Prompt after formatting:\n",
      "\u001B[32;1m\u001B[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
      "\n",
      "Current conversation:\n",
      "New summary: James greets the AI and introduces himself. The AI responds warmly, welcoming him and offering assistance with various tasks. The human then asks about the different types of conversational memory, and the AI explains that it refers to the cognitive ability to retain and recall context, references, sequences, intentions, and nuances in dialogue. The AI outlines categories such as contextual, referential, temporal, intentional, episodic/semantic, prosodic, and pragmatic memory, while noting that it is studied in cognitive psychology, linguistics, and AI/NLP. Key factors influencing it include age, cognitive load, and cultural context. The AI invites further exploration of specific frameworks if needed.\n",
      "Human: I have been looking at ConversationBufferMemory and ConversationBufferWindowMemory.\n",
      "AI:\u001B[0m\n",
      "\n",
      "\u001B[1m> Finished chain.\u001B[0m\n",
      "\n",
      "\n",
      "\u001B[1m> Entering new ConversationChain chain...\u001B[0m\n",
      "Prompt after formatting:\n",
      "\u001B[32;1m\u001B[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
      "\n",
      "Current conversation:\n",
      "New summary: James greets the AI and introduces himself. The AI responds warmly, welcoming him and offering assistance with various tasks. The human then asks about the different types of conversational memory, and the AI explains that it refers to the cognitive ability to retain and recall context, references, sequences, intentions, and nuances in dialogue. The AI outlines categories such as contextual, referential, temporal, intentional, episodic/semantic, prosodic, and pragmatic memory, while noting that it is studied in cognitive psychology, linguistics, and AI/NLP. Key factors influencing it include age, cognitive load, and cultural context. The AI invites further exploration of specific frameworks if needed. The human then mentions examining **`ConversationBufferMemory`** and **`ConversationBufferWindowMemory`**, prompting the AI to explain these memory mechanisms in conversational AI systems. The AI details their differences: **`ConversationBufferMemory`** stores all conversation history for long-term context retention, ideal for multi-turn dialogues but potentially memory-intensive, while **`ConversationBufferWindowMemory`** retains only the most recent N messages (via a window size) for efficiency and focus on recent context. The AI highlights trade-offs, such as the risk of losing critical information with the latter, and provides use cases for each. A comparison table summarizes their scope, window size, use cases, and risks. The AI concludes by asking if the user wants code examples (e.g., in Python with LangChain) or details on how these memory types interact with retrieval systems like vector databases.\n",
      "Human: Buffer memory just stores the entire conversation\n",
      "AI:\u001B[0m\n",
      "\n",
      "\u001B[1m> Finished chain.\u001B[0m\n",
      "\n",
      "\n",
      "\u001B[1m> Entering new ConversationChain chain...\u001B[0m\n",
      "Prompt after formatting:\n",
      "\u001B[32;1m\u001B[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
      "\n",
      "Current conversation:\n",
      "New summary: James greets the AI and introduces himself. The AI responds warmly, welcoming him and offering assistance with various tasks. The human then asks about the different types of conversational memory, and the AI explains that it refers to the cognitive ability to retain and recall context, references, sequences, intentions, and nuances in dialogue. The AI outlines categories such as contextual, referential, temporal, intentional, episodic/semantic, prosodic, and pragmatic memory, while noting that it is studied in cognitive psychology, linguistics, and AI/NLP. Key factors influencing it include age, cognitive load, and cultural context. The AI invites further exploration of specific frameworks if needed. The human then mentions examining **`ConversationBufferMemory`** and **`ConversationBufferWindowMemory`**, prompting the AI to explain these memory mechanisms in conversational AI systems. The AI details their differences: **`ConversationBufferMemory`** stores all conversation history for long-term context retention, ideal for multi-turn dialogues but potentially memory-intensive, while **`ConversationBufferWindowMemory`** retains only the most recent N messages (via a window size) for efficiency and focus on recent context. The AI highlights trade-offs, such as the risk of losing critical information with the latter, and provides use cases for each. A comparison table summarizes their scope, window size, use cases, and risks. The AI concludes by asking if the user wants code examples (e.g., in Python with LangChain) or details on how these memory types interact with retrieval systems like vector databases. The human confirms that buffer memory stores the entire conversation, and the AI elaborates that **`ConversationBufferMemory`** is designed to retain full history for critical contextual continuity but may become memory-intensive. It contrasts this with **`ConversationBufferWindowMemory`**, which prioritizes recent interactions via a sliding window, balancing efficiency with context retention. The AI concludes by offering code examples or further clarification on implementation.\n",
      "Human: Buffer window memory stores the last k messages, dropping the rest.\n",
      "AI:\u001B[0m\n",
      "\n",
      "\u001B[1m> Finished chain.\u001B[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'input': 'Buffer window memory stores the last k messages, dropping the rest.',\n",
       " 'history': 'New summary: James greets the AI and introduces himself. The AI responds warmly, welcoming him and offering assistance with various tasks. The human then asks about the different types of conversational memory, and the AI explains that it refers to the cognitive ability to retain and recall context, references, sequences, intentions, and nuances in dialogue. The AI outlines categories such as contextual, referential, temporal, intentional, episodic/semantic, prosodic, and pragmatic memory, while noting that it is studied in cognitive psychology, linguistics, and AI/NLP. Key factors influencing it include age, cognitive load, and cultural context. The AI invites further exploration of specific frameworks if needed. The human then mentions examining **`ConversationBufferMemory`** and **`ConversationBufferWindowMemory`**, prompting the AI to explain these memory mechanisms in conversational AI systems. The AI details their differences: **`ConversationBufferMemory`** stores all conversation history for long-term context retention, ideal for multi-turn dialogues but potentially memory-intensive, while **`ConversationBufferWindowMemory`** retains only the most recent N messages (via a window size) for efficiency and focus on recent context. The AI highlights trade-offs, such as the risk of losing critical information with the latter, and provides use cases for each. A comparison table summarizes their scope, window size, use cases, and risks. The AI concludes by asking if the user wants code examples (e.g., in Python with LangChain) or details on how these memory types interact with retrieval systems like vector databases. The human confirms that buffer memory stores the entire conversation, and the AI elaborates that **`ConversationBufferMemory`** is designed to retain full history for critical contextual continuity but may become memory-intensive. It contrasts this with **`ConversationBufferWindowMemory`**, which prioritizes recent interactions via a sliding window, balancing efficiency with context retention. The AI concludes by offering code examples or further clarification on implementation.',\n",
       " 'response': \"You're absolutely right! **`ConversationBufferWindowMemory`** operates by maintaining a sliding window of the most recent `k` messages, effectively discarding older interactions beyond that window. This approach is efficient for scenarios where the model needs to focus on **recent context** while avoiding memory overload. For example, in a customer service chatbot, it might retain the last 5 messages to address the current query without being distracted by earlier, less relevant details. \\n\\nHowever, this method **sacrifices long-term context** for efficiency. If the conversation requires referencing earlier parts (e.g., tracking a multi-step task), the window size must be carefully chosen. Would you like to explore how to implement this in code (e.g., with LangChain) or discuss trade-offs for specific use cases? ðŸ˜Š\"}"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 56
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-03-01T12:34:53.789619Z",
     "start_time": "2026-03-01T12:34:42.350995Z"
    }
   },
   "cell_type": "code",
   "source": "chain.invoke({\"input\": \"What is my name again?\"})",
   "id": "f2005ec38c35e88c",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001B[1m> Entering new ConversationChain chain...\u001B[0m\n",
      "Prompt after formatting:\n",
      "\u001B[32;1m\u001B[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
      "\n",
      "Current conversation:\n",
      "New summary: James greets the AI and introduces himself. The AI responds warmly, welcoming him and offering assistance with various tasks. The human then asks about the different types of conversational memory, and the AI explains that it refers to the cognitive ability to retain and recall context, references, sequences, intentions, and nuances in dialogue. The AI outlines categories such as contextual, referential, temporal, intentional, episodic/semantic, prosodic, and pragmatic memory, while noting that it is studied in cognitive psychology, linguistics, and AI/NLP. Key factors influencing it include age, cognitive load, and cultural context. The AI invites further exploration of specific frameworks if needed. The human then mentions examining **`ConversationBufferMemory`** and **`ConversationBufferWindowMemory`**, prompting the AI to explain these memory mechanisms in conversational AI systems. The AI details their differences: **`ConversationBufferMemory`** stores all conversation history for long-term context retention, ideal for multi-turn dialogues but potentially memory-intensive, while **`ConversationBufferWindowMemory`** retains only the most recent N messages (via a window size) for efficiency and focus on recent context. The AI highlights trade-offs, such as the risk of losing critical information with the latter, and provides use cases for each. A comparison table summarizes their scope, window size, use cases, and risks. The AI concludes by asking if the user wants code examples (e.g., in Python with LangChain) or details on how these memory types interact with retrieval systems like vector databases. The human confirms that buffer memory stores the entire conversation, and the AI elaborates that **`ConversationMemory`** retains full history for critical contextual continuity but may become memory-intensive. It contrasts this with **`ConversationBufferWindowMemory`**, which prioritizes recent interactions via a sliding window of the last `k` messages, discarding older interactions. The AI explains that this approach is efficient for scenarios like customer service chatbots, where focusing on recent context avoids memory overload, but sacrifices long-term context for efficiency. The AI notes that window size must be carefully chosen if the conversation requires referencing earlier parts (e.g., multi-step tasks) and asks if the user wants code examples or to discuss trade-offs for specific use cases.\n",
      "Human: What is my name again?\n",
      "AI:\u001B[0m\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[31m---------------------------------------------------------------------------\u001B[39m",
      "\u001B[31mKeyboardInterrupt\u001B[39m                         Traceback (most recent call last)",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[57]\u001B[39m\u001B[32m, line 1\u001B[39m\n\u001B[32m----> \u001B[39m\u001B[32m1\u001B[39m \u001B[43mchain\u001B[49m\u001B[43m.\u001B[49m\u001B[43minvoke\u001B[49m\u001B[43m(\u001B[49m\u001B[43m{\u001B[49m\u001B[33;43m\"\u001B[39;49m\u001B[33;43minput\u001B[39;49m\u001B[33;43m\"\u001B[39;49m\u001B[43m:\u001B[49m\u001B[43m \u001B[49m\u001B[33;43m\"\u001B[39;49m\u001B[33;43mWhat is my name again?\u001B[39;49m\u001B[33;43m\"\u001B[39;49m\u001B[43m}\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/pet_projects/ai/langchain-course/.venv/lib/python3.12/site-packages/langchain_classic/chains/base.py:167\u001B[39m, in \u001B[36mChain.invoke\u001B[39m\u001B[34m(self, input, config, **kwargs)\u001B[39m\n\u001B[32m    164\u001B[39m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[32m    165\u001B[39m     \u001B[38;5;28mself\u001B[39m._validate_inputs(inputs)\n\u001B[32m    166\u001B[39m     outputs = (\n\u001B[32m--> \u001B[39m\u001B[32m167\u001B[39m         \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43m_call\u001B[49m\u001B[43m(\u001B[49m\u001B[43minputs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mrun_manager\u001B[49m\u001B[43m=\u001B[49m\u001B[43mrun_manager\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    168\u001B[39m         \u001B[38;5;28;01mif\u001B[39;00m new_arg_supported\n\u001B[32m    169\u001B[39m         \u001B[38;5;28;01melse\u001B[39;00m \u001B[38;5;28mself\u001B[39m._call(inputs)\n\u001B[32m    170\u001B[39m     )\n\u001B[32m    172\u001B[39m     final_outputs: \u001B[38;5;28mdict\u001B[39m[\u001B[38;5;28mstr\u001B[39m, Any] = \u001B[38;5;28mself\u001B[39m.prep_outputs(\n\u001B[32m    173\u001B[39m         inputs,\n\u001B[32m    174\u001B[39m         outputs,\n\u001B[32m    175\u001B[39m         return_only_outputs,\n\u001B[32m    176\u001B[39m     )\n\u001B[32m    177\u001B[39m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mBaseException\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m e:\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/pet_projects/ai/langchain-course/.venv/lib/python3.12/site-packages/langchain_classic/chains/llm.py:117\u001B[39m, in \u001B[36mLLMChain._call\u001B[39m\u001B[34m(self, inputs, run_manager)\u001B[39m\n\u001B[32m    112\u001B[39m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34m_call\u001B[39m(\n\u001B[32m    113\u001B[39m     \u001B[38;5;28mself\u001B[39m,\n\u001B[32m    114\u001B[39m     inputs: \u001B[38;5;28mdict\u001B[39m[\u001B[38;5;28mstr\u001B[39m, Any],\n\u001B[32m    115\u001B[39m     run_manager: CallbackManagerForChainRun | \u001B[38;5;28;01mNone\u001B[39;00m = \u001B[38;5;28;01mNone\u001B[39;00m,\n\u001B[32m    116\u001B[39m ) -> \u001B[38;5;28mdict\u001B[39m[\u001B[38;5;28mstr\u001B[39m, \u001B[38;5;28mstr\u001B[39m]:\n\u001B[32m--> \u001B[39m\u001B[32m117\u001B[39m     response = \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43mgenerate\u001B[49m\u001B[43m(\u001B[49m\u001B[43m[\u001B[49m\u001B[43minputs\u001B[49m\u001B[43m]\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mrun_manager\u001B[49m\u001B[43m=\u001B[49m\u001B[43mrun_manager\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    118\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m.create_outputs(response)[\u001B[32m0\u001B[39m]\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/pet_projects/ai/langchain-course/.venv/lib/python3.12/site-packages/langchain_classic/chains/llm.py:129\u001B[39m, in \u001B[36mLLMChain.generate\u001B[39m\u001B[34m(self, input_list, run_manager)\u001B[39m\n\u001B[32m    127\u001B[39m callbacks = run_manager.get_child() \u001B[38;5;28;01mif\u001B[39;00m run_manager \u001B[38;5;28;01melse\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[32m    128\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(\u001B[38;5;28mself\u001B[39m.llm, BaseLanguageModel):\n\u001B[32m--> \u001B[39m\u001B[32m129\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43mllm\u001B[49m\u001B[43m.\u001B[49m\u001B[43mgenerate_prompt\u001B[49m\u001B[43m(\u001B[49m\n\u001B[32m    130\u001B[39m \u001B[43m        \u001B[49m\u001B[43mprompts\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    131\u001B[39m \u001B[43m        \u001B[49m\u001B[43mstop\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    132\u001B[39m \u001B[43m        \u001B[49m\u001B[43mcallbacks\u001B[49m\u001B[43m=\u001B[49m\u001B[43mcallbacks\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    133\u001B[39m \u001B[43m        \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43mllm_kwargs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    134\u001B[39m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    135\u001B[39m results = \u001B[38;5;28mself\u001B[39m.llm.bind(stop=stop, **\u001B[38;5;28mself\u001B[39m.llm_kwargs).batch(\n\u001B[32m    136\u001B[39m     cast(\u001B[33m\"\u001B[39m\u001B[33mlist\u001B[39m\u001B[33m\"\u001B[39m, prompts),\n\u001B[32m    137\u001B[39m     {\u001B[33m\"\u001B[39m\u001B[33mcallbacks\u001B[39m\u001B[33m\"\u001B[39m: callbacks},\n\u001B[32m    138\u001B[39m )\n\u001B[32m    139\u001B[39m generations: \u001B[38;5;28mlist\u001B[39m[\u001B[38;5;28mlist\u001B[39m[Generation]] = []\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/pet_projects/ai/langchain-course/.venv/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py:1121\u001B[39m, in \u001B[36mBaseChatModel.generate_prompt\u001B[39m\u001B[34m(self, prompts, stop, callbacks, **kwargs)\u001B[39m\n\u001B[32m   1112\u001B[39m \u001B[38;5;129m@override\u001B[39m\n\u001B[32m   1113\u001B[39m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34mgenerate_prompt\u001B[39m(\n\u001B[32m   1114\u001B[39m     \u001B[38;5;28mself\u001B[39m,\n\u001B[32m   (...)\u001B[39m\u001B[32m   1118\u001B[39m     **kwargs: Any,\n\u001B[32m   1119\u001B[39m ) -> LLMResult:\n\u001B[32m   1120\u001B[39m     prompt_messages = [p.to_messages() \u001B[38;5;28;01mfor\u001B[39;00m p \u001B[38;5;129;01min\u001B[39;00m prompts]\n\u001B[32m-> \u001B[39m\u001B[32m1121\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43mgenerate\u001B[49m\u001B[43m(\u001B[49m\u001B[43mprompt_messages\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mstop\u001B[49m\u001B[43m=\u001B[49m\u001B[43mstop\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mcallbacks\u001B[49m\u001B[43m=\u001B[49m\u001B[43mcallbacks\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/pet_projects/ai/langchain-course/.venv/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py:931\u001B[39m, in \u001B[36mBaseChatModel.generate\u001B[39m\u001B[34m(self, messages, stop, callbacks, tags, metadata, run_name, run_id, **kwargs)\u001B[39m\n\u001B[32m    928\u001B[39m \u001B[38;5;28;01mfor\u001B[39;00m i, m \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28menumerate\u001B[39m(input_messages):\n\u001B[32m    929\u001B[39m     \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[32m    930\u001B[39m         results.append(\n\u001B[32m--> \u001B[39m\u001B[32m931\u001B[39m             \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43m_generate_with_cache\u001B[49m\u001B[43m(\u001B[49m\n\u001B[32m    932\u001B[39m \u001B[43m                \u001B[49m\u001B[43mm\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    933\u001B[39m \u001B[43m                \u001B[49m\u001B[43mstop\u001B[49m\u001B[43m=\u001B[49m\u001B[43mstop\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    934\u001B[39m \u001B[43m                \u001B[49m\u001B[43mrun_manager\u001B[49m\u001B[43m=\u001B[49m\u001B[43mrun_managers\u001B[49m\u001B[43m[\u001B[49m\u001B[43mi\u001B[49m\u001B[43m]\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43;01mif\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[43mrun_managers\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43;01melse\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[38;5;28;43;01mNone\u001B[39;49;00m\u001B[43m,\u001B[49m\n\u001B[32m    935\u001B[39m \u001B[43m                \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mkwargs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    936\u001B[39m \u001B[43m            \u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    937\u001B[39m         )\n\u001B[32m    938\u001B[39m     \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mBaseException\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m e:\n\u001B[32m    939\u001B[39m         \u001B[38;5;28;01mif\u001B[39;00m run_managers:\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/pet_projects/ai/langchain-course/.venv/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py:1233\u001B[39m, in \u001B[36mBaseChatModel._generate_with_cache\u001B[39m\u001B[34m(self, messages, stop, run_manager, **kwargs)\u001B[39m\n\u001B[32m   1231\u001B[39m     result = generate_from_stream(\u001B[38;5;28miter\u001B[39m(chunks))\n\u001B[32m   1232\u001B[39m \u001B[38;5;28;01melif\u001B[39;00m inspect.signature(\u001B[38;5;28mself\u001B[39m._generate).parameters.get(\u001B[33m\"\u001B[39m\u001B[33mrun_manager\u001B[39m\u001B[33m\"\u001B[39m):\n\u001B[32m-> \u001B[39m\u001B[32m1233\u001B[39m     result = \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43m_generate\u001B[49m\u001B[43m(\u001B[49m\n\u001B[32m   1234\u001B[39m \u001B[43m        \u001B[49m\u001B[43mmessages\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mstop\u001B[49m\u001B[43m=\u001B[49m\u001B[43mstop\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mrun_manager\u001B[49m\u001B[43m=\u001B[49m\u001B[43mrun_manager\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mkwargs\u001B[49m\n\u001B[32m   1235\u001B[39m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m   1236\u001B[39m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[32m   1237\u001B[39m     result = \u001B[38;5;28mself\u001B[39m._generate(messages, stop=stop, **kwargs)\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/pet_projects/ai/langchain-course/.venv/lib/python3.12/site-packages/langchain_ollama/chat_models.py:1030\u001B[39m, in \u001B[36mChatOllama._generate\u001B[39m\u001B[34m(self, messages, stop, run_manager, **kwargs)\u001B[39m\n\u001B[32m   1023\u001B[39m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34m_generate\u001B[39m(\n\u001B[32m   1024\u001B[39m     \u001B[38;5;28mself\u001B[39m,\n\u001B[32m   1025\u001B[39m     messages: \u001B[38;5;28mlist\u001B[39m[BaseMessage],\n\u001B[32m   (...)\u001B[39m\u001B[32m   1028\u001B[39m     **kwargs: Any,\n\u001B[32m   1029\u001B[39m ) -> ChatResult:\n\u001B[32m-> \u001B[39m\u001B[32m1030\u001B[39m     final_chunk = \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43m_chat_stream_with_aggregation\u001B[49m\u001B[43m(\u001B[49m\n\u001B[32m   1031\u001B[39m \u001B[43m        \u001B[49m\u001B[43mmessages\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mstop\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mrun_manager\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mverbose\u001B[49m\u001B[43m=\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43mverbose\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mkwargs\u001B[49m\n\u001B[32m   1032\u001B[39m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m   1033\u001B[39m     generation_info = final_chunk.generation_info\n\u001B[32m   1034\u001B[39m     chat_generation = ChatGeneration(\n\u001B[32m   1035\u001B[39m         message=AIMessage(\n\u001B[32m   1036\u001B[39m             content=final_chunk.text,\n\u001B[32m   (...)\u001B[39m\u001B[32m   1043\u001B[39m         generation_info=generation_info,\n\u001B[32m   1044\u001B[39m     )\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/pet_projects/ai/langchain-course/.venv/lib/python3.12/site-packages/langchain_ollama/chat_models.py:965\u001B[39m, in \u001B[36mChatOllama._chat_stream_with_aggregation\u001B[39m\u001B[34m(self, messages, stop, run_manager, verbose, **kwargs)\u001B[39m\n\u001B[32m    956\u001B[39m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34m_chat_stream_with_aggregation\u001B[39m(\n\u001B[32m    957\u001B[39m     \u001B[38;5;28mself\u001B[39m,\n\u001B[32m    958\u001B[39m     messages: \u001B[38;5;28mlist\u001B[39m[BaseMessage],\n\u001B[32m   (...)\u001B[39m\u001B[32m    962\u001B[39m     **kwargs: Any,\n\u001B[32m    963\u001B[39m ) -> ChatGenerationChunk:\n\u001B[32m    964\u001B[39m     final_chunk = \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[32m--> \u001B[39m\u001B[32m965\u001B[39m \u001B[43m    \u001B[49m\u001B[38;5;28;43;01mfor\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[43mchunk\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;129;43;01min\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43m_iterate_over_stream\u001B[49m\u001B[43m(\u001B[49m\u001B[43mmessages\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mstop\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\u001B[43m:\u001B[49m\n\u001B[32m    966\u001B[39m \u001B[43m        \u001B[49m\u001B[38;5;28;43;01mif\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[43mfinal_chunk\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;129;43;01mis\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[38;5;28;43;01mNone\u001B[39;49;00m\u001B[43m:\u001B[49m\n\u001B[32m    967\u001B[39m \u001B[43m            \u001B[49m\u001B[43mfinal_chunk\u001B[49m\u001B[43m \u001B[49m\u001B[43m=\u001B[49m\u001B[43m \u001B[49m\u001B[43mchunk\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/pet_projects/ai/langchain-course/.venv/lib/python3.12/site-packages/langchain_ollama/chat_models.py:1054\u001B[39m, in \u001B[36mChatOllama._iterate_over_stream\u001B[39m\u001B[34m(self, messages, stop, **kwargs)\u001B[39m\n\u001B[32m   1047\u001B[39m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34m_iterate_over_stream\u001B[39m(\n\u001B[32m   1048\u001B[39m     \u001B[38;5;28mself\u001B[39m,\n\u001B[32m   1049\u001B[39m     messages: \u001B[38;5;28mlist\u001B[39m[BaseMessage],\n\u001B[32m   1050\u001B[39m     stop: \u001B[38;5;28mlist\u001B[39m[\u001B[38;5;28mstr\u001B[39m] | \u001B[38;5;28;01mNone\u001B[39;00m = \u001B[38;5;28;01mNone\u001B[39;00m,\n\u001B[32m   1051\u001B[39m     **kwargs: Any,\n\u001B[32m   1052\u001B[39m ) -> Iterator[ChatGenerationChunk]:\n\u001B[32m   1053\u001B[39m     reasoning = kwargs.get(\u001B[33m\"\u001B[39m\u001B[33mreasoning\u001B[39m\u001B[33m\"\u001B[39m, \u001B[38;5;28mself\u001B[39m.reasoning)\n\u001B[32m-> \u001B[39m\u001B[32m1054\u001B[39m \u001B[43m    \u001B[49m\u001B[38;5;28;43;01mfor\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[43mstream_resp\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;129;43;01min\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43m_create_chat_stream\u001B[49m\u001B[43m(\u001B[49m\u001B[43mmessages\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mstop\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\u001B[43m:\u001B[49m\n\u001B[32m   1055\u001B[39m \u001B[43m        \u001B[49m\u001B[38;5;28;43;01mif\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[38;5;129;43;01mnot\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[38;5;28;43misinstance\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43mstream_resp\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mstr\u001B[39;49m\u001B[43m)\u001B[49m\u001B[43m:\u001B[49m\n\u001B[32m   1056\u001B[39m \u001B[43m            \u001B[49m\u001B[43mcontent\u001B[49m\u001B[43m \u001B[49m\u001B[43m=\u001B[49m\u001B[43m \u001B[49m\u001B[43m(\u001B[49m\n\u001B[32m   1057\u001B[39m \u001B[43m                \u001B[49m\u001B[43mstream_resp\u001B[49m\u001B[43m[\u001B[49m\u001B[33;43m\"\u001B[39;49m\u001B[33;43mmessage\u001B[39;49m\u001B[33;43m\"\u001B[39;49m\u001B[43m]\u001B[49m\u001B[43m[\u001B[49m\u001B[33;43m\"\u001B[39;49m\u001B[33;43mcontent\u001B[39;49m\u001B[33;43m\"\u001B[39;49m\u001B[43m]\u001B[49m\n\u001B[32m   1058\u001B[39m \u001B[43m                \u001B[49m\u001B[38;5;28;43;01mif\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[33;43m\"\u001B[39;49m\u001B[33;43mmessage\u001B[39;49m\u001B[33;43m\"\u001B[39;49m\u001B[43m \u001B[49m\u001B[38;5;129;43;01min\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[43mstream_resp\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;129;43;01mand\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[33;43m\"\u001B[39;49m\u001B[33;43mcontent\u001B[39;49m\u001B[33;43m\"\u001B[39;49m\u001B[43m \u001B[49m\u001B[38;5;129;43;01min\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[43mstream_resp\u001B[49m\u001B[43m[\u001B[49m\u001B[33;43m\"\u001B[39;49m\u001B[33;43mmessage\u001B[39;49m\u001B[33;43m\"\u001B[39;49m\u001B[43m]\u001B[49m\n\u001B[32m   1059\u001B[39m \u001B[43m                \u001B[49m\u001B[38;5;28;43;01melse\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[33;43m\"\u001B[39;49m\u001B[33;43m\"\u001B[39;49m\n\u001B[32m   1060\u001B[39m \u001B[43m            \u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/pet_projects/ai/langchain-course/.venv/lib/python3.12/site-packages/langchain_ollama/chat_models.py:952\u001B[39m, in \u001B[36mChatOllama._create_chat_stream\u001B[39m\u001B[34m(self, messages, stop, **kwargs)\u001B[39m\n\u001B[32m    950\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m chat_params[\u001B[33m\"\u001B[39m\u001B[33mstream\u001B[39m\u001B[33m\"\u001B[39m]:\n\u001B[32m    951\u001B[39m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m._client:\n\u001B[32m--> \u001B[39m\u001B[32m952\u001B[39m         \u001B[38;5;28;01myield from\u001B[39;00m \u001B[38;5;28mself\u001B[39m._client.chat(**chat_params)\n\u001B[32m    953\u001B[39m \u001B[38;5;28;01melif\u001B[39;00m \u001B[38;5;28mself\u001B[39m._client:\n\u001B[32m    954\u001B[39m     \u001B[38;5;28;01myield\u001B[39;00m \u001B[38;5;28mself\u001B[39m._client.chat(**chat_params)\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/pet_projects/ai/langchain-course/.venv/lib/python3.12/site-packages/ollama/_client.py:181\u001B[39m, in \u001B[36mClient._request.<locals>.inner\u001B[39m\u001B[34m()\u001B[39m\n\u001B[32m    178\u001B[39m   e.response.read()\n\u001B[32m    179\u001B[39m   \u001B[38;5;28;01mraise\u001B[39;00m ResponseError(e.response.text, e.response.status_code) \u001B[38;5;28;01mfrom\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[32m--> \u001B[39m\u001B[32m181\u001B[39m \u001B[43m\u001B[49m\u001B[38;5;28;43;01mfor\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[43mline\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;129;43;01min\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[43mr\u001B[49m\u001B[43m.\u001B[49m\u001B[43miter_lines\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\u001B[43m:\u001B[49m\n\u001B[32m    182\u001B[39m \u001B[43m  \u001B[49m\u001B[43mpart\u001B[49m\u001B[43m \u001B[49m\u001B[43m=\u001B[49m\u001B[43m \u001B[49m\u001B[43mjson\u001B[49m\u001B[43m.\u001B[49m\u001B[43mloads\u001B[49m\u001B[43m(\u001B[49m\u001B[43mline\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    183\u001B[39m \u001B[43m  \u001B[49m\u001B[38;5;28;43;01mif\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[43merr\u001B[49m\u001B[43m \u001B[49m\u001B[43m:=\u001B[49m\u001B[43m \u001B[49m\u001B[43mpart\u001B[49m\u001B[43m.\u001B[49m\u001B[43mget\u001B[49m\u001B[43m(\u001B[49m\u001B[33;43m'\u001B[39;49m\u001B[33;43merror\u001B[39;49m\u001B[33;43m'\u001B[39;49m\u001B[43m)\u001B[49m\u001B[43m:\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/pet_projects/ai/langchain-course/.venv/lib/python3.12/site-packages/httpx/_models.py:929\u001B[39m, in \u001B[36mResponse.iter_lines\u001B[39m\u001B[34m(self)\u001B[39m\n\u001B[32m    927\u001B[39m decoder = LineDecoder()\n\u001B[32m    928\u001B[39m \u001B[38;5;28;01mwith\u001B[39;00m request_context(request=\u001B[38;5;28mself\u001B[39m._request):\n\u001B[32m--> \u001B[39m\u001B[32m929\u001B[39m \u001B[43m    \u001B[49m\u001B[38;5;28;43;01mfor\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[43mtext\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;129;43;01min\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43miter_text\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\u001B[43m:\u001B[49m\n\u001B[32m    930\u001B[39m \u001B[43m        \u001B[49m\u001B[38;5;28;43;01mfor\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[43mline\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;129;43;01min\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[43mdecoder\u001B[49m\u001B[43m.\u001B[49m\u001B[43mdecode\u001B[49m\u001B[43m(\u001B[49m\u001B[43mtext\u001B[49m\u001B[43m)\u001B[49m\u001B[43m:\u001B[49m\n\u001B[32m    931\u001B[39m \u001B[43m            \u001B[49m\u001B[38;5;28;43;01myield\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[43mline\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/pet_projects/ai/langchain-course/.venv/lib/python3.12/site-packages/httpx/_models.py:916\u001B[39m, in \u001B[36mResponse.iter_text\u001B[39m\u001B[34m(self, chunk_size)\u001B[39m\n\u001B[32m    914\u001B[39m chunker = TextChunker(chunk_size=chunk_size)\n\u001B[32m    915\u001B[39m \u001B[38;5;28;01mwith\u001B[39;00m request_context(request=\u001B[38;5;28mself\u001B[39m._request):\n\u001B[32m--> \u001B[39m\u001B[32m916\u001B[39m \u001B[43m    \u001B[49m\u001B[38;5;28;43;01mfor\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[43mbyte_content\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;129;43;01min\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43miter_bytes\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\u001B[43m:\u001B[49m\n\u001B[32m    917\u001B[39m \u001B[43m        \u001B[49m\u001B[43mtext_content\u001B[49m\u001B[43m \u001B[49m\u001B[43m=\u001B[49m\u001B[43m \u001B[49m\u001B[43mdecoder\u001B[49m\u001B[43m.\u001B[49m\u001B[43mdecode\u001B[49m\u001B[43m(\u001B[49m\u001B[43mbyte_content\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    918\u001B[39m \u001B[43m        \u001B[49m\u001B[38;5;28;43;01mfor\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[43mchunk\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;129;43;01min\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[43mchunker\u001B[49m\u001B[43m.\u001B[49m\u001B[43mdecode\u001B[49m\u001B[43m(\u001B[49m\u001B[43mtext_content\u001B[49m\u001B[43m)\u001B[49m\u001B[43m:\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/pet_projects/ai/langchain-course/.venv/lib/python3.12/site-packages/httpx/_models.py:897\u001B[39m, in \u001B[36mResponse.iter_bytes\u001B[39m\u001B[34m(self, chunk_size)\u001B[39m\n\u001B[32m    895\u001B[39m chunker = ByteChunker(chunk_size=chunk_size)\n\u001B[32m    896\u001B[39m \u001B[38;5;28;01mwith\u001B[39;00m request_context(request=\u001B[38;5;28mself\u001B[39m._request):\n\u001B[32m--> \u001B[39m\u001B[32m897\u001B[39m \u001B[43m    \u001B[49m\u001B[38;5;28;43;01mfor\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[43mraw_bytes\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;129;43;01min\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43miter_raw\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\u001B[43m:\u001B[49m\n\u001B[32m    898\u001B[39m \u001B[43m        \u001B[49m\u001B[43mdecoded\u001B[49m\u001B[43m \u001B[49m\u001B[43m=\u001B[49m\u001B[43m \u001B[49m\u001B[43mdecoder\u001B[49m\u001B[43m.\u001B[49m\u001B[43mdecode\u001B[49m\u001B[43m(\u001B[49m\u001B[43mraw_bytes\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    899\u001B[39m \u001B[43m        \u001B[49m\u001B[38;5;28;43;01mfor\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[43mchunk\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;129;43;01min\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[43mchunker\u001B[49m\u001B[43m.\u001B[49m\u001B[43mdecode\u001B[49m\u001B[43m(\u001B[49m\u001B[43mdecoded\u001B[49m\u001B[43m)\u001B[49m\u001B[43m:\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/pet_projects/ai/langchain-course/.venv/lib/python3.12/site-packages/httpx/_models.py:951\u001B[39m, in \u001B[36mResponse.iter_raw\u001B[39m\u001B[34m(self, chunk_size)\u001B[39m\n\u001B[32m    948\u001B[39m chunker = ByteChunker(chunk_size=chunk_size)\n\u001B[32m    950\u001B[39m \u001B[38;5;28;01mwith\u001B[39;00m request_context(request=\u001B[38;5;28mself\u001B[39m._request):\n\u001B[32m--> \u001B[39m\u001B[32m951\u001B[39m \u001B[43m    \u001B[49m\u001B[38;5;28;43;01mfor\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[43mraw_stream_bytes\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;129;43;01min\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43mstream\u001B[49m\u001B[43m:\u001B[49m\n\u001B[32m    952\u001B[39m \u001B[43m        \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43m_num_bytes_downloaded\u001B[49m\u001B[43m \u001B[49m\u001B[43m+\u001B[49m\u001B[43m=\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mlen\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43mraw_stream_bytes\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    953\u001B[39m \u001B[43m        \u001B[49m\u001B[38;5;28;43;01mfor\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[43mchunk\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;129;43;01min\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[43mchunker\u001B[49m\u001B[43m.\u001B[49m\u001B[43mdecode\u001B[49m\u001B[43m(\u001B[49m\u001B[43mraw_stream_bytes\u001B[49m\u001B[43m)\u001B[49m\u001B[43m:\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/pet_projects/ai/langchain-course/.venv/lib/python3.12/site-packages/httpx/_client.py:153\u001B[39m, in \u001B[36mBoundSyncStream.__iter__\u001B[39m\u001B[34m(self)\u001B[39m\n\u001B[32m    152\u001B[39m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34m__iter__\u001B[39m(\u001B[38;5;28mself\u001B[39m) -> typing.Iterator[\u001B[38;5;28mbytes\u001B[39m]:\n\u001B[32m--> \u001B[39m\u001B[32m153\u001B[39m \u001B[43m    \u001B[49m\u001B[38;5;28;43;01mfor\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[43mchunk\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;129;43;01min\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43m_stream\u001B[49m\u001B[43m:\u001B[49m\n\u001B[32m    154\u001B[39m \u001B[43m        \u001B[49m\u001B[38;5;28;43;01myield\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[43mchunk\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/pet_projects/ai/langchain-course/.venv/lib/python3.12/site-packages/httpx/_transports/default.py:127\u001B[39m, in \u001B[36mResponseStream.__iter__\u001B[39m\u001B[34m(self)\u001B[39m\n\u001B[32m    125\u001B[39m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34m__iter__\u001B[39m(\u001B[38;5;28mself\u001B[39m) -> typing.Iterator[\u001B[38;5;28mbytes\u001B[39m]:\n\u001B[32m    126\u001B[39m     \u001B[38;5;28;01mwith\u001B[39;00m map_httpcore_exceptions():\n\u001B[32m--> \u001B[39m\u001B[32m127\u001B[39m \u001B[43m        \u001B[49m\u001B[38;5;28;43;01mfor\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[43mpart\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;129;43;01min\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43m_httpcore_stream\u001B[49m\u001B[43m:\u001B[49m\n\u001B[32m    128\u001B[39m \u001B[43m            \u001B[49m\u001B[38;5;28;43;01myield\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[43mpart\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/pet_projects/ai/langchain-course/.venv/lib/python3.12/site-packages/httpcore/_sync/connection_pool.py:407\u001B[39m, in \u001B[36mPoolByteStream.__iter__\u001B[39m\u001B[34m(self)\u001B[39m\n\u001B[32m    405\u001B[39m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mBaseException\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m exc:\n\u001B[32m    406\u001B[39m     \u001B[38;5;28mself\u001B[39m.close()\n\u001B[32m--> \u001B[39m\u001B[32m407\u001B[39m     \u001B[38;5;28;01mraise\u001B[39;00m exc \u001B[38;5;28;01mfrom\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;28;01mNone\u001B[39;00m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/pet_projects/ai/langchain-course/.venv/lib/python3.12/site-packages/httpcore/_sync/connection_pool.py:403\u001B[39m, in \u001B[36mPoolByteStream.__iter__\u001B[39m\u001B[34m(self)\u001B[39m\n\u001B[32m    401\u001B[39m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34m__iter__\u001B[39m(\u001B[38;5;28mself\u001B[39m) -> typing.Iterator[\u001B[38;5;28mbytes\u001B[39m]:\n\u001B[32m    402\u001B[39m     \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[32m--> \u001B[39m\u001B[32m403\u001B[39m \u001B[43m        \u001B[49m\u001B[38;5;28;43;01mfor\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[43mpart\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;129;43;01min\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43m_stream\u001B[49m\u001B[43m:\u001B[49m\n\u001B[32m    404\u001B[39m \u001B[43m            \u001B[49m\u001B[38;5;28;43;01myield\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[43mpart\u001B[49m\n\u001B[32m    405\u001B[39m     \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mBaseException\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m exc:\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/pet_projects/ai/langchain-course/.venv/lib/python3.12/site-packages/httpcore/_sync/http11.py:342\u001B[39m, in \u001B[36mHTTP11ConnectionByteStream.__iter__\u001B[39m\u001B[34m(self)\u001B[39m\n\u001B[32m    340\u001B[39m \u001B[38;5;28;01mwith\u001B[39;00m ShieldCancellation():\n\u001B[32m    341\u001B[39m     \u001B[38;5;28mself\u001B[39m.close()\n\u001B[32m--> \u001B[39m\u001B[32m342\u001B[39m \u001B[38;5;28;01mraise\u001B[39;00m exc\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/pet_projects/ai/langchain-course/.venv/lib/python3.12/site-packages/httpcore/_sync/http11.py:334\u001B[39m, in \u001B[36mHTTP11ConnectionByteStream.__iter__\u001B[39m\u001B[34m(self)\u001B[39m\n\u001B[32m    332\u001B[39m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[32m    333\u001B[39m     \u001B[38;5;28;01mwith\u001B[39;00m Trace(\u001B[33m\"\u001B[39m\u001B[33mreceive_response_body\u001B[39m\u001B[33m\"\u001B[39m, logger, \u001B[38;5;28mself\u001B[39m._request, kwargs):\n\u001B[32m--> \u001B[39m\u001B[32m334\u001B[39m \u001B[43m        \u001B[49m\u001B[38;5;28;43;01mfor\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[43mchunk\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;129;43;01min\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43m_connection\u001B[49m\u001B[43m.\u001B[49m\u001B[43m_receive_response_body\u001B[49m\u001B[43m(\u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\u001B[43m:\u001B[49m\n\u001B[32m    335\u001B[39m \u001B[43m            \u001B[49m\u001B[38;5;28;43;01myield\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[43mchunk\u001B[49m\n\u001B[32m    336\u001B[39m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mBaseException\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m exc:\n\u001B[32m    337\u001B[39m     \u001B[38;5;66;03m# If we get an exception while streaming the response,\u001B[39;00m\n\u001B[32m    338\u001B[39m     \u001B[38;5;66;03m# we want to close the response (and possibly the connection)\u001B[39;00m\n\u001B[32m    339\u001B[39m     \u001B[38;5;66;03m# before raising that exception.\u001B[39;00m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/pet_projects/ai/langchain-course/.venv/lib/python3.12/site-packages/httpcore/_sync/http11.py:203\u001B[39m, in \u001B[36mHTTP11Connection._receive_response_body\u001B[39m\u001B[34m(self, request)\u001B[39m\n\u001B[32m    200\u001B[39m timeout = timeouts.get(\u001B[33m\"\u001B[39m\u001B[33mread\u001B[39m\u001B[33m\"\u001B[39m, \u001B[38;5;28;01mNone\u001B[39;00m)\n\u001B[32m    202\u001B[39m \u001B[38;5;28;01mwhile\u001B[39;00m \u001B[38;5;28;01mTrue\u001B[39;00m:\n\u001B[32m--> \u001B[39m\u001B[32m203\u001B[39m     event = \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43m_receive_event\u001B[49m\u001B[43m(\u001B[49m\u001B[43mtimeout\u001B[49m\u001B[43m=\u001B[49m\u001B[43mtimeout\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    204\u001B[39m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(event, h11.Data):\n\u001B[32m    205\u001B[39m         \u001B[38;5;28;01myield\u001B[39;00m \u001B[38;5;28mbytes\u001B[39m(event.data)\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/pet_projects/ai/langchain-course/.venv/lib/python3.12/site-packages/httpcore/_sync/http11.py:217\u001B[39m, in \u001B[36mHTTP11Connection._receive_event\u001B[39m\u001B[34m(self, timeout)\u001B[39m\n\u001B[32m    214\u001B[39m     event = \u001B[38;5;28mself\u001B[39m._h11_state.next_event()\n\u001B[32m    216\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m event \u001B[38;5;129;01mis\u001B[39;00m h11.NEED_DATA:\n\u001B[32m--> \u001B[39m\u001B[32m217\u001B[39m     data = \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43m_network_stream\u001B[49m\u001B[43m.\u001B[49m\u001B[43mread\u001B[49m\u001B[43m(\u001B[49m\n\u001B[32m    218\u001B[39m \u001B[43m        \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43mREAD_NUM_BYTES\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtimeout\u001B[49m\u001B[43m=\u001B[49m\u001B[43mtimeout\u001B[49m\n\u001B[32m    219\u001B[39m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    221\u001B[39m     \u001B[38;5;66;03m# If we feed this case through h11 we'll raise an exception like:\u001B[39;00m\n\u001B[32m    222\u001B[39m     \u001B[38;5;66;03m#\u001B[39;00m\n\u001B[32m    223\u001B[39m     \u001B[38;5;66;03m#     httpcore.RemoteProtocolError: can't handle event type\u001B[39;00m\n\u001B[32m   (...)\u001B[39m\u001B[32m    227\u001B[39m     \u001B[38;5;66;03m# perspective. Instead we handle this case distinctly and treat\u001B[39;00m\n\u001B[32m    228\u001B[39m     \u001B[38;5;66;03m# it as a ConnectError.\u001B[39;00m\n\u001B[32m    229\u001B[39m     \u001B[38;5;28;01mif\u001B[39;00m data == \u001B[33mb\u001B[39m\u001B[33m\"\u001B[39m\u001B[33m\"\u001B[39m \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;28mself\u001B[39m._h11_state.their_state == h11.SEND_RESPONSE:\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/pet_projects/ai/langchain-course/.venv/lib/python3.12/site-packages/httpcore/_backends/sync.py:128\u001B[39m, in \u001B[36mSyncStream.read\u001B[39m\u001B[34m(self, max_bytes, timeout)\u001B[39m\n\u001B[32m    126\u001B[39m \u001B[38;5;28;01mwith\u001B[39;00m map_exceptions(exc_map):\n\u001B[32m    127\u001B[39m     \u001B[38;5;28mself\u001B[39m._sock.settimeout(timeout)\n\u001B[32m--> \u001B[39m\u001B[32m128\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43m_sock\u001B[49m\u001B[43m.\u001B[49m\u001B[43mrecv\u001B[49m\u001B[43m(\u001B[49m\u001B[43mmax_bytes\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[31mKeyboardInterrupt\u001B[39m: "
     ]
    }
   ],
   "execution_count": 57
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-03-01T13:00:58.230242Z",
     "start_time": "2026-03-01T13:00:58.213543Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from langchain_core.messages import SystemMessage\n",
    "from langchain_ollama import ChatOllama\n",
    "\n",
    "class ConversationSummaryMessageHistory(BaseChatMessageHistory, BaseModel):\n",
    "    messages: list[BaseMessage] = Field(default_factory=list)\n",
    "    llm: ChatOllama = Field(default_factory=ChatOllama)\n",
    "\n",
    "    def __init__(self, llm: ChatOllama):\n",
    "        super().__init__(llm=llm)\n",
    "\n",
    "    def add_messages(self, messages: list[BaseMessage]) -> None:\n",
    "        \"\"\"Add messages to the history, removing any messages beyond\n",
    "        the last `k` messages.\n",
    "        \"\"\"\n",
    "        self.messages.extend(messages)\n",
    "        # construct the summary chat messages\n",
    "        summary_prompt = ChatPromptTemplate.from_messages([\n",
    "            SystemMessagePromptTemplate.from_template(\n",
    "                \"Given the existing conversation summary and the new messages, \"\n",
    "                \"generate a new summary of the conversation. Ensuring to maintain \"\n",
    "                \"as much relevant information as possible.\"\n",
    "            ),\n",
    "            HumanMessagePromptTemplate.from_template(\n",
    "                \"Existing conversation summary:\\n{existing_summary}\\n\\n\"\n",
    "                \"New messages:\\n{messages}\"\n",
    "            )\n",
    "        ])\n",
    "        # format the messages and invoke the LLM\n",
    "        new_summary = self.llm.invoke(\n",
    "            summary_prompt.format_messages(\n",
    "                existing_summary=self.messages,\n",
    "                messages=[x.content for x in messages]\n",
    "            )\n",
    "        )\n",
    "        # replace the existing history with a single system summary message\n",
    "        self.messages = [SystemMessage(content=new_summary.content)]\n",
    "\n",
    "    def clear(self) -> None:\n",
    "        \"\"\"Clear the history.\"\"\"\n",
    "        self.messages = []"
   ],
   "id": "b4ca8c898cb08a75",
   "outputs": [],
   "execution_count": 71
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-03-01T13:01:00.327792Z",
     "start_time": "2026-03-01T13:01:00.312767Z"
    }
   },
   "cell_type": "code",
   "source": [
    "chat_map = {}\n",
    "def get_chat_history(session_id:str, llm: ChatOllama) -> ConversationSummaryMessageHistory:\n",
    "    if session_id not in chat_map:\n",
    "        chat_map[session_id] = ConversationSummaryMessageHistory(llm)\n",
    "    return chat_map[session_id]"
   ],
   "id": "9f5ec53e44b172d7",
   "outputs": [],
   "execution_count": 72
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-03-01T13:01:01.667185Z",
     "start_time": "2026-03-01T13:01:01.655940Z"
    }
   },
   "cell_type": "code",
   "source": [
    "pipeline_with_history = RunnableWithMessageHistory(\n",
    "    pipeline,\n",
    "    get_session_history=get_chat_history,\n",
    "    input_messages_key=\"query\",\n",
    "    history_messages_key=\"history\",\n",
    "    history_factory_config=[\n",
    "        ConfigurableFieldSpec(\n",
    "            id=\"session_id\",\n",
    "            annotation=str,\n",
    "            name=\"Session ID\",\n",
    "            description=\"The session ID to use for the chat history\",\n",
    "            default=\"id_default\",\n",
    "        ),\n",
    "        ConfigurableFieldSpec(\n",
    "            id=\"llm\",\n",
    "            annotation=ChatOllama,\n",
    "            name=\"LLM\",\n",
    "            description=\"The LLM to use for the conversation summary\",\n",
    "            default=llm,\n",
    "        )\n",
    "    ]\n",
    ")"
   ],
   "id": "8d4831869627e6ed",
   "outputs": [],
   "execution_count": 73
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-03-01T13:02:54.577744Z",
     "start_time": "2026-03-01T13:01:03.379701Z"
    }
   },
   "cell_type": "code",
   "source": [
    "pipeline_with_history.invoke(\n",
    "    {\"query\": \"Hi, my name is James\"},\n",
    "    config={\"session_id\": \"id_123\", \"llm\": llm}\n",
    ")"
   ],
   "id": "4398559fd401145a",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content=\"Hello, James! It's a pleasure to meet you. How are you doing today? I'd love to hear more about you or anything you'd like to discuss. ðŸ˜Š\", additional_kwargs={}, response_metadata={'model': 'qwen3:8b', 'created_at': '2026-03-01T13:01:18.653269Z', 'done': True, 'done_reason': 'stop', 'total_duration': 15213884917, 'load_duration': 103474125, 'prompt_eval_count': 29, 'prompt_eval_duration': 1848615750, 'eval_count': 135, 'eval_duration': 13122294578, 'logprobs': None, 'model_name': 'qwen3:8b', 'model_provider': 'ollama'}, id='lc_run--019ca97c-f04d-74b3-b7df-3e0123ee7b31-0', tool_calls=[], invalid_tool_calls=[], usage_metadata={'input_tokens': 29, 'output_tokens': 135, 'total_tokens': 164})"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 74
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-03-01T13:03:03.458868Z",
     "start_time": "2026-03-01T13:03:03.427140Z"
    }
   },
   "cell_type": "code",
   "source": "chat_map[\"id_123\"].messages",
   "id": "8f1404d0c8e4da85",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[SystemMessage(content='**New Conversation Summary:**  \\nJames introduced himself with \"Hi, my name is James.\" The AI responded warmly, saying, \"Hello, James! It\\'s a pleasure to meet you. How are you doing today? I\\'d love to hear more about you or anything you\\'d like to discuss. ðŸ˜Š\" The conversation remains brief and friendly, with no further exchanges provided.', additional_kwargs={}, response_metadata={})]"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 75
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-03-01T13:07:26.629558Z",
     "start_time": "2026-03-01T13:03:10.020696Z"
    }
   },
   "cell_type": "code",
   "source": [
    "pipeline_with_history.invoke(\n",
    "    {\"query\": \"I'm researching the different types of conversational memory.\"},\n",
    "    config={\"session_id\": \"id_123\", \"llm\": llm}\n",
    ")\n",
    "\n",
    "chat_map[\"id_123\"].messages"
   ],
   "id": "c2b9ff80fe9cb099",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[SystemMessage(content='**New Conversation Summary:**  \\nJames initiated the conversation by introducing himself with \"Hi, my name is James.\" The AI responded warmly, welcoming him and expressing eagerness to learn about his interests. James then shared his research focus on *conversational memory*, which the AI elaborated on by explaining its role in retaining and processing information during dialogue. The AI categorized conversational memory into four main types: **Functional**, **Temporal**, **Structural**, and **Cognitive Components**, each with subcategories like *Memory for Content*, *Memory for Context*, and *Working Memory*. The AI also highlighted key brain regions (e.g., prefrontal cortex, temporal lobes) and discussed challenges like Alzheimerâ€™s or cultural differences in memory prioritization. The conversation concluded with the AI inviting further exploration of the topic. ðŸ˜Š  \\n\\n*(Note: The new summary integrates the existing dialogue and the detailed explanation of conversational memory types provided in the AIMessage, ensuring all relevant information is preserved without redundancy.)*', additional_kwargs={}, response_metadata={})]"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 76
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-03-01T13:14:59.939391Z",
     "start_time": "2026-03-01T13:08:04.167648Z"
    }
   },
   "cell_type": "code",
   "source": [
    "for msg in [\n",
    "    \"I have been looking at ConversationBufferMemory and ConversationBufferWindowMemory.\",\n",
    "    \"Buffer memory just stores the entire conversation\",\n",
    "    \"Buffer window memory stores the last k messages, dropping the rest.\"\n",
    "]:\n",
    "    pipeline_with_history.invoke(\n",
    "        {\"query\": msg},\n",
    "        config={\"session_id\": \"id_123\", \"llm\": llm}\n",
    "    )"
   ],
   "id": "4c19ac64c9436eea",
   "outputs": [],
   "execution_count": 77
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-03-01T13:15:05.706408Z",
     "start_time": "2026-03-01T13:15:05.664633Z"
    }
   },
   "cell_type": "code",
   "source": "chat_map[\"id_123\"].messages",
   "id": "b03999d147b7bf75",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[SystemMessage(content='**New Conversation Summary:**  \\nJames initiated the discussion by explaining his research on *conversational memory*, which the AI expanded upon by detailing its role in retaining and processing information during dialogue. The AI outlined four types of conversational memory: **Functional**, **Temporal**, **Structural**, and **Cognitive Components**, with subcategories like *Memory for Content*, *Memory for Context*, and *Working Memory*. Key brain regions (e.g., prefrontal cortex, temporal lobes) and challenges like Alzheimerâ€™s or cultural differences in memory prioritization were also addressed.  \\n\\nThe conversation then focused on **`ConversationBufferMemory`** and **`ConversationBufferWindowMemory`**. James noted that *Buffer memory stores the entire conversation*, a point the AI affirmed, explaining that **`ConversationBufferMemory`** acts as a complete history log, ideal for short interactions requiring full context (e.g., troubleshooting, step-by-step guidance). However, it can become inefficient for long dialogues due to unbounded memory growth.  \\n\\nIn contrast, **`ConversationBufferWindowMemory`** retains only the *last `k` messages* (controlled by the `window_size` parameter), discarding older ones. The AI emphasized that this approach is ideal for **longer conversations** where retaining the **most recent context** is critical, while avoiding memory bloat. Key trade-offs include:  \\n- **`BufferMemory`**: Simplicity for short, focused interactions but risks resource strain over time.  \\n- **`BufferWindowMemory`**: Scalability for extended dialogues (e.g., multi-turn conversations, complex tasks) but may lose older context if the window size is too small.  \\n\\nThe AI concluded by asking if James needed guidance on selecting between the two based on his use case. ðŸ˜Š  \\n\\n*(Note: This summary integrates the original discussion on conversational memory with the new exploration of memory mechanisms in conversational AI, emphasizing key distinctions, trade-offs, and practical considerations.)*', additional_kwargs={}, response_metadata={})]"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 78
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-03-01T13:18:01.260070Z",
     "start_time": "2026-03-01T13:15:30.176001Z"
    }
   },
   "cell_type": "code",
   "source": [
    "pipeline_with_history.invoke(\n",
    "    {\"query\": \"What is my name again?\"},\n",
    "    config={\"session_id\": \"id_123\", \"llm\": llm}\n",
    ")"
   ],
   "id": "b07e116cb1af1d65",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content=\"I don't have access to your name unless you've shared it with me in our conversation. Could you please tell me your name? ðŸ˜Š\", additional_kwargs={}, response_metadata={'model': 'qwen3:8b', 'created_at': '2026-03-01T13:16:21.911934Z', 'done': True, 'done_reason': 'stop', 'total_duration': 51674016250, 'load_duration': 123232958, 'prompt_eval_count': 432, 'prompt_eval_duration': 5205930833, 'eval_count': 463, 'eval_duration': 45848488709, 'logprobs': None, 'model_name': 'qwen3:8b', 'model_provider': 'ollama'}, id='lc_run--019ca98a-2a17-73f2-84de-e437aced899b-0', tool_calls=[], invalid_tool_calls=[], usage_metadata={'input_tokens': 432, 'output_tokens': 463, 'total_tokens': 895})"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 79
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-03-01T13:20:22.211870Z",
     "start_time": "2026-03-01T13:20:22.154169Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from  langchain_classic.memory import ConversationSummaryBufferMemory\n",
    "\n",
    "memory = ConversationSummaryBufferMemory(\n",
    "    llm=llm,\n",
    "    max_token_limit=300,\n",
    "    return_messages=True\n",
    ")"
   ],
   "id": "32fccdaede43faff",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/dc/v08dhs892w5ffjzk9_wrf6800000gn/T/ipykernel_38463/2205579329.py:3: LangChainDeprecationWarning: Please see the migration guide at: https://python.langchain.com/docs/versions/migrating_memory/\n",
      "  memory = ConversationSummaryBufferMemory(\n"
     ]
    }
   ],
   "execution_count": 80
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-03-01T13:22:56.619388Z",
     "start_time": "2026-03-01T13:22:56.587048Z"
    }
   },
   "cell_type": "code",
   "source": [
    "chain = ConversationChain(\n",
    "    llm=llm,\n",
    "    memory=memory,\n",
    "    verbose=True\n",
    ")"
   ],
   "id": "f372e2b9bf336d69",
   "outputs": [],
   "execution_count": 83
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-03-01T13:25:56.529746Z",
     "start_time": "2026-03-01T13:25:24.687933Z"
    }
   },
   "cell_type": "code",
   "source": "chain.invoke({\"input\": \"Hi, my name is James\"})",
   "id": "fbb67ab94e31247c",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001B[1m> Entering new ConversationChain chain...\u001B[0m\n",
      "Prompt after formatting:\n",
      "\u001B[32;1m\u001B[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
      "\n",
      "Current conversation:\n",
      "[HumanMessage(content='Hi, my name is James', additional_kwargs={}, response_metadata={}), AIMessage(content=\"Hello, James! It's great to meet you. I'm an AI assistant here to help with any questions or tasks you might have. How are you doing today? ðŸ˜Š\", additional_kwargs={}, response_metadata={}, tool_calls=[], invalid_tool_calls=[]), HumanMessage(content='Hi, my name is James', additional_kwargs={}, response_metadata={}), AIMessage(content=\"Hello again, James! ðŸ˜Š I'm glad to hear from you. How are you doing today? Is there anything specific you'd like to discuss or need assistance with?\", additional_kwargs={}, response_metadata={}, tool_calls=[], invalid_tool_calls=[]), HumanMessage(content=\"I'm researching the different types of conversational memory.\", additional_kwargs={}, response_metadata={}), AIMessage(content='Conversational memory refers to the cognitive ability to retain and recall the context, details, and flow of a conversation to maintain coherence and continuity in communication. While it isn\\'t a formally classified \"type\" of memory in psychology, it involves several key components and processes:\\n\\n1. **Context Maintenance**: Keeping track of the conversation\\'s topic, participants, and prior statements to avoid confusion or repetition. For example, remembering that a previous discussion was about climate change helps guide the next response.\\n\\n2. **Turn-Taking Awareness**: Recognizing who is speaking and when, which requires short-term memory to manage the rhythm of dialogue (e.g., knowing when to pause or respond).\\n\\n3. **Episodic Memory Integration**: Drawing on personal experiences or past interactions to enrich the conversation (e.g., recalling a shared memory to make the dialogue more engaging).\\n\\n4. **Working Memory**: Temporarily holding and manipulating information during the conversation, such as processing new details while referencing earlier points.\\n\\n5. **Semantic Memory**: Using factual knowledge to support the conversation (e.g., recalling historical events or scientific concepts).\\n\\nIn AI systems, conversational memory is often implemented through context management tools that track user inputs and previous interactions to maintain a coherent dialogue. For humans, this ability is part of broader working memory and episodic memory systems. If you\\'re exploring specific frameworks or theories, let me knowâ€”Iâ€™d be happy to dive deeper! ðŸ˜Š', additional_kwargs={}, response_metadata={}, tool_calls=[], invalid_tool_calls=[])]\n",
      "Human: Hi, my name is James\n",
      "AI:\u001B[0m\n"
     ]
    },
    {
     "ename": "ImportError",
     "evalue": "Could not import transformers python package. This is needed in order to calculate get_token_ids. Please install it with `pip install transformers`.",
     "output_type": "error",
     "traceback": [
      "\u001B[31m---------------------------------------------------------------------------\u001B[39m",
      "\u001B[31mImportError\u001B[39m                               Traceback (most recent call last)",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[86]\u001B[39m\u001B[32m, line 1\u001B[39m\n\u001B[32m----> \u001B[39m\u001B[32m1\u001B[39m \u001B[43mchain\u001B[49m\u001B[43m.\u001B[49m\u001B[43minvoke\u001B[49m\u001B[43m(\u001B[49m\u001B[43m{\u001B[49m\u001B[33;43m\"\u001B[39;49m\u001B[33;43minput\u001B[39;49m\u001B[33;43m\"\u001B[39;49m\u001B[43m:\u001B[49m\u001B[43m \u001B[49m\u001B[33;43m\"\u001B[39;49m\u001B[33;43mHi, my name is James\u001B[39;49m\u001B[33;43m\"\u001B[39;49m\u001B[43m}\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/pet_projects/ai/langchain-course/.venv/lib/python3.12/site-packages/langchain_classic/chains/base.py:172\u001B[39m, in \u001B[36mChain.invoke\u001B[39m\u001B[34m(self, input, config, **kwargs)\u001B[39m\n\u001B[32m    165\u001B[39m     \u001B[38;5;28mself\u001B[39m._validate_inputs(inputs)\n\u001B[32m    166\u001B[39m     outputs = (\n\u001B[32m    167\u001B[39m         \u001B[38;5;28mself\u001B[39m._call(inputs, run_manager=run_manager)\n\u001B[32m    168\u001B[39m         \u001B[38;5;28;01mif\u001B[39;00m new_arg_supported\n\u001B[32m    169\u001B[39m         \u001B[38;5;28;01melse\u001B[39;00m \u001B[38;5;28mself\u001B[39m._call(inputs)\n\u001B[32m    170\u001B[39m     )\n\u001B[32m--> \u001B[39m\u001B[32m172\u001B[39m     final_outputs: \u001B[38;5;28mdict\u001B[39m[\u001B[38;5;28mstr\u001B[39m, Any] = \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43mprep_outputs\u001B[49m\u001B[43m(\u001B[49m\n\u001B[32m    173\u001B[39m \u001B[43m        \u001B[49m\u001B[43minputs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    174\u001B[39m \u001B[43m        \u001B[49m\u001B[43moutputs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    175\u001B[39m \u001B[43m        \u001B[49m\u001B[43mreturn_only_outputs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    176\u001B[39m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    177\u001B[39m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mBaseException\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m e:\n\u001B[32m    178\u001B[39m     run_manager.on_chain_error(e)\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/pet_projects/ai/langchain-course/.venv/lib/python3.12/site-packages/langchain_classic/chains/base.py:491\u001B[39m, in \u001B[36mChain.prep_outputs\u001B[39m\u001B[34m(self, inputs, outputs, return_only_outputs)\u001B[39m\n\u001B[32m    489\u001B[39m \u001B[38;5;28mself\u001B[39m._validate_outputs(outputs)\n\u001B[32m    490\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m.memory \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[32m--> \u001B[39m\u001B[32m491\u001B[39m     \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43mmemory\u001B[49m\u001B[43m.\u001B[49m\u001B[43msave_context\u001B[49m\u001B[43m(\u001B[49m\u001B[43minputs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43moutputs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    492\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m return_only_outputs:\n\u001B[32m    493\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m outputs\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/pet_projects/ai/langchain-course/.venv/lib/python3.12/site-packages/langchain_classic/memory/summary_buffer.py:101\u001B[39m, in \u001B[36mConversationSummaryBufferMemory.save_context\u001B[39m\u001B[34m(self, inputs, outputs)\u001B[39m\n\u001B[32m     99\u001B[39m \u001B[38;5;250m\u001B[39m\u001B[33;03m\"\"\"Save context from this conversation to buffer.\"\"\"\u001B[39;00m\n\u001B[32m    100\u001B[39m \u001B[38;5;28msuper\u001B[39m().save_context(inputs, outputs)\n\u001B[32m--> \u001B[39m\u001B[32m101\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43mprune\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/pet_projects/ai/langchain-course/.venv/lib/python3.12/site-packages/langchain_classic/memory/summary_buffer.py:115\u001B[39m, in \u001B[36mConversationSummaryBufferMemory.prune\u001B[39m\u001B[34m(self)\u001B[39m\n\u001B[32m    113\u001B[39m \u001B[38;5;250m\u001B[39m\u001B[33;03m\"\"\"Prune buffer if it exceeds max token limit.\"\"\"\u001B[39;00m\n\u001B[32m    114\u001B[39m buffer = \u001B[38;5;28mself\u001B[39m.chat_memory.messages\n\u001B[32m--> \u001B[39m\u001B[32m115\u001B[39m curr_buffer_length = \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43mllm\u001B[49m\u001B[43m.\u001B[49m\u001B[43mget_num_tokens_from_messages\u001B[49m\u001B[43m(\u001B[49m\u001B[43mbuffer\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    116\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m curr_buffer_length > \u001B[38;5;28mself\u001B[39m.max_token_limit:\n\u001B[32m    117\u001B[39m     pruned_memory = []\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/pet_projects/ai/langchain-course/.venv/lib/python3.12/site-packages/langchain_core/language_models/base.py:373\u001B[39m, in \u001B[36mBaseLanguageModel.get_num_tokens_from_messages\u001B[39m\u001B[34m(self, messages, tools)\u001B[39m\n\u001B[32m    368\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m tools \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[32m    369\u001B[39m     warnings.warn(\n\u001B[32m    370\u001B[39m         \u001B[33m\"\u001B[39m\u001B[33mCounting tokens in tool schemas is not yet supported. Ignoring tools.\u001B[39m\u001B[33m\"\u001B[39m,\n\u001B[32m    371\u001B[39m         stacklevel=\u001B[32m2\u001B[39m,\n\u001B[32m    372\u001B[39m     )\n\u001B[32m--> \u001B[39m\u001B[32m373\u001B[39m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43msum\u001B[39;49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43mget_num_tokens\u001B[49m\u001B[43m(\u001B[49m\u001B[43mget_buffer_string\u001B[49m\u001B[43m(\u001B[49m\u001B[43m[\u001B[49m\u001B[43mm\u001B[49m\u001B[43m]\u001B[49m\u001B[43m)\u001B[49m\u001B[43m)\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43;01mfor\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[43mm\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;129;43;01min\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[43mmessages\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/pet_projects/ai/langchain-course/.venv/lib/python3.12/site-packages/langchain_core/language_models/base.py:373\u001B[39m, in \u001B[36m<genexpr>\u001B[39m\u001B[34m(.0)\u001B[39m\n\u001B[32m    368\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m tools \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[32m    369\u001B[39m     warnings.warn(\n\u001B[32m    370\u001B[39m         \u001B[33m\"\u001B[39m\u001B[33mCounting tokens in tool schemas is not yet supported. Ignoring tools.\u001B[39m\u001B[33m\"\u001B[39m,\n\u001B[32m    371\u001B[39m         stacklevel=\u001B[32m2\u001B[39m,\n\u001B[32m    372\u001B[39m     )\n\u001B[32m--> \u001B[39m\u001B[32m373\u001B[39m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28msum\u001B[39m(\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43mget_num_tokens\u001B[49m\u001B[43m(\u001B[49m\u001B[43mget_buffer_string\u001B[49m\u001B[43m(\u001B[49m\u001B[43m[\u001B[49m\u001B[43mm\u001B[49m\u001B[43m]\u001B[49m\u001B[43m)\u001B[49m\u001B[43m)\u001B[49m \u001B[38;5;28;01mfor\u001B[39;00m m \u001B[38;5;129;01min\u001B[39;00m messages)\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/pet_projects/ai/langchain-course/.venv/lib/python3.12/site-packages/langchain_core/language_models/base.py:336\u001B[39m, in \u001B[36mBaseLanguageModel.get_num_tokens\u001B[39m\u001B[34m(self, text)\u001B[39m\n\u001B[32m    321\u001B[39m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34mget_num_tokens\u001B[39m(\u001B[38;5;28mself\u001B[39m, text: \u001B[38;5;28mstr\u001B[39m) -> \u001B[38;5;28mint\u001B[39m:\n\u001B[32m    322\u001B[39m \u001B[38;5;250m    \u001B[39m\u001B[33;03m\"\"\"Get the number of tokens present in the text.\u001B[39;00m\n\u001B[32m    323\u001B[39m \n\u001B[32m    324\u001B[39m \u001B[33;03m    Useful for checking if an input fits in a model's context window.\u001B[39;00m\n\u001B[32m   (...)\u001B[39m\u001B[32m    334\u001B[39m \n\u001B[32m    335\u001B[39m \u001B[33;03m    \"\"\"\u001B[39;00m\n\u001B[32m--> \u001B[39m\u001B[32m336\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mlen\u001B[39m(\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43mget_token_ids\u001B[49m\u001B[43m(\u001B[49m\u001B[43mtext\u001B[49m\u001B[43m)\u001B[49m)\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/pet_projects/ai/langchain-course/.venv/lib/python3.12/site-packages/langchain_core/language_models/base.py:319\u001B[39m, in \u001B[36mBaseLanguageModel.get_token_ids\u001B[39m\u001B[34m(self, text)\u001B[39m\n\u001B[32m    317\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m.custom_get_token_ids \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[32m    318\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m.custom_get_token_ids(text)\n\u001B[32m--> \u001B[39m\u001B[32m319\u001B[39m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43m_get_token_ids_default_method\u001B[49m\u001B[43m(\u001B[49m\u001B[43mtext\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/pet_projects/ai/langchain-course/.venv/lib/python3.12/site-packages/langchain_core/language_models/base.py:113\u001B[39m, in \u001B[36m_get_token_ids_default_method\u001B[39m\u001B[34m(text)\u001B[39m\n\u001B[32m    105\u001B[39m     warnings.warn(\n\u001B[32m    106\u001B[39m         \u001B[33m\"\u001B[39m\u001B[33mUsing fallback GPT-2 tokenizer for token counting. \u001B[39m\u001B[33m\"\u001B[39m\n\u001B[32m    107\u001B[39m         \u001B[33m\"\u001B[39m\u001B[33mToken counts may be inaccurate for non-GPT-2 models. \u001B[39m\u001B[33m\"\u001B[39m\n\u001B[32m    108\u001B[39m         \u001B[33m\"\u001B[39m\u001B[33mFor accurate counts, use a model-specific method if available.\u001B[39m\u001B[33m\"\u001B[39m,\n\u001B[32m    109\u001B[39m         stacklevel=\u001B[32m3\u001B[39m,\n\u001B[32m    110\u001B[39m     )\n\u001B[32m    111\u001B[39m     _GPT2_TOKENIZER_WARNED = \u001B[38;5;28;01mTrue\u001B[39;00m\n\u001B[32m--> \u001B[39m\u001B[32m113\u001B[39m tokenizer = \u001B[43mget_tokenizer\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    115\u001B[39m \u001B[38;5;66;03m# Pass verbose=False to suppress the \"Token indices sequence length is longer than\u001B[39;00m\n\u001B[32m    116\u001B[39m \u001B[38;5;66;03m# the specified maximum sequence length\" warning from HuggingFace. This warning is\u001B[39;00m\n\u001B[32m    117\u001B[39m \u001B[38;5;66;03m# about GPT-2's 1024 token context limit, but we're only using the tokenizer for\u001B[39;00m\n\u001B[32m    118\u001B[39m \u001B[38;5;66;03m# counting, not for model input.\u001B[39;00m\n\u001B[32m    119\u001B[39m \u001B[38;5;28;01mreturn\u001B[39;00m cast(\u001B[33m\"\u001B[39m\u001B[33mlist[int]\u001B[39m\u001B[33m\"\u001B[39m, tokenizer.encode(text, verbose=\u001B[38;5;28;01mFalse\u001B[39;00m))\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/pet_projects/ai/langchain-course/.venv/lib/python3.12/site-packages/langchain_core/language_models/base.py:93\u001B[39m, in \u001B[36mget_tokenizer\u001B[39m\u001B[34m()\u001B[39m\n\u001B[32m     87\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m _HAS_TRANSFORMERS:\n\u001B[32m     88\u001B[39m     msg = (\n\u001B[32m     89\u001B[39m         \u001B[33m\"\u001B[39m\u001B[33mCould not import transformers python package. \u001B[39m\u001B[33m\"\u001B[39m\n\u001B[32m     90\u001B[39m         \u001B[33m\"\u001B[39m\u001B[33mThis is needed in order to calculate get_token_ids. \u001B[39m\u001B[33m\"\u001B[39m\n\u001B[32m     91\u001B[39m         \u001B[33m\"\u001B[39m\u001B[33mPlease install it with `pip install transformers`.\u001B[39m\u001B[33m\"\u001B[39m\n\u001B[32m     92\u001B[39m     )\n\u001B[32m---> \u001B[39m\u001B[32m93\u001B[39m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mImportError\u001B[39;00m(msg)\n\u001B[32m     94\u001B[39m \u001B[38;5;66;03m# create a GPT-2 tokenizer instance\u001B[39;00m\n\u001B[32m     95\u001B[39m \u001B[38;5;28;01mreturn\u001B[39;00m GPT2TokenizerFast.from_pretrained(\u001B[33m\"\u001B[39m\u001B[33mgpt2\u001B[39m\u001B[33m\"\u001B[39m)\n",
      "\u001B[31mImportError\u001B[39m: Could not import transformers python package. This is needed in order to calculate get_token_ids. Please install it with `pip install transformers`."
     ]
    }
   ],
   "execution_count": 86
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-03-01T13:24:57.459240Z",
     "start_time": "2026-03-01T13:23:22.715356Z"
    }
   },
   "cell_type": "code",
   "source": [
    "for msg in [\n",
    "    \"I'm researching the different types of conversational memory.\",\n",
    "    \"I have been looking at ConversationBufferMemory and ConversationBufferWindowMemory.\",\n",
    "    \"Buffer memory just stores the entire conversation\",\n",
    "    \"Buffer window memory stores the last k messages, dropping the rest.\"\n",
    "]:\n",
    "    chain.invoke({\"input\": msg})"
   ],
   "id": "718b2920cf5a9a0f",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001B[1m> Entering new ConversationChain chain...\u001B[0m\n",
      "Prompt after formatting:\n",
      "\u001B[32;1m\u001B[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
      "\n",
      "Current conversation:\n",
      "[HumanMessage(content='Hi, my name is James', additional_kwargs={}, response_metadata={}), AIMessage(content=\"Hello, James! It's great to meet you. I'm an AI assistant here to help with any questions or tasks you might have. How are you doing today? ðŸ˜Š\", additional_kwargs={}, response_metadata={}, tool_calls=[], invalid_tool_calls=[]), HumanMessage(content='Hi, my name is James', additional_kwargs={}, response_metadata={}), AIMessage(content=\"Hello again, James! ðŸ˜Š I'm glad to hear from you. How are you doing today? Is there anything specific you'd like to discuss or need assistance with?\", additional_kwargs={}, response_metadata={}, tool_calls=[], invalid_tool_calls=[])]\n",
      "Human: I'm researching the different types of conversational memory.\n",
      "AI:\u001B[0m\n"
     ]
    },
    {
     "ename": "ImportError",
     "evalue": "Could not import transformers python package. This is needed in order to calculate get_token_ids. Please install it with `pip install transformers`.",
     "output_type": "error",
     "traceback": [
      "\u001B[31m---------------------------------------------------------------------------\u001B[39m",
      "\u001B[31mImportError\u001B[39m                               Traceback (most recent call last)",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[85]\u001B[39m\u001B[32m, line 7\u001B[39m\n\u001B[32m      1\u001B[39m \u001B[38;5;28;01mfor\u001B[39;00m msg \u001B[38;5;129;01min\u001B[39;00m [\n\u001B[32m      2\u001B[39m     \u001B[33m\"\u001B[39m\u001B[33mI\u001B[39m\u001B[33m'\u001B[39m\u001B[33mm researching the different types of conversational memory.\u001B[39m\u001B[33m\"\u001B[39m,\n\u001B[32m      3\u001B[39m     \u001B[33m\"\u001B[39m\u001B[33mI have been looking at ConversationBufferMemory and ConversationBufferWindowMemory.\u001B[39m\u001B[33m\"\u001B[39m,\n\u001B[32m      4\u001B[39m     \u001B[33m\"\u001B[39m\u001B[33mBuffer memory just stores the entire conversation\u001B[39m\u001B[33m\"\u001B[39m,\n\u001B[32m      5\u001B[39m     \u001B[33m\"\u001B[39m\u001B[33mBuffer window memory stores the last k messages, dropping the rest.\u001B[39m\u001B[33m\"\u001B[39m\n\u001B[32m      6\u001B[39m ]:\n\u001B[32m----> \u001B[39m\u001B[32m7\u001B[39m     \u001B[43mchain\u001B[49m\u001B[43m.\u001B[49m\u001B[43minvoke\u001B[49m\u001B[43m(\u001B[49m\u001B[43m{\u001B[49m\u001B[33;43m\"\u001B[39;49m\u001B[33;43minput\u001B[39;49m\u001B[33;43m\"\u001B[39;49m\u001B[43m:\u001B[49m\u001B[43m \u001B[49m\u001B[43mmsg\u001B[49m\u001B[43m}\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/pet_projects/ai/langchain-course/.venv/lib/python3.12/site-packages/langchain_classic/chains/base.py:172\u001B[39m, in \u001B[36mChain.invoke\u001B[39m\u001B[34m(self, input, config, **kwargs)\u001B[39m\n\u001B[32m    165\u001B[39m     \u001B[38;5;28mself\u001B[39m._validate_inputs(inputs)\n\u001B[32m    166\u001B[39m     outputs = (\n\u001B[32m    167\u001B[39m         \u001B[38;5;28mself\u001B[39m._call(inputs, run_manager=run_manager)\n\u001B[32m    168\u001B[39m         \u001B[38;5;28;01mif\u001B[39;00m new_arg_supported\n\u001B[32m    169\u001B[39m         \u001B[38;5;28;01melse\u001B[39;00m \u001B[38;5;28mself\u001B[39m._call(inputs)\n\u001B[32m    170\u001B[39m     )\n\u001B[32m--> \u001B[39m\u001B[32m172\u001B[39m     final_outputs: \u001B[38;5;28mdict\u001B[39m[\u001B[38;5;28mstr\u001B[39m, Any] = \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43mprep_outputs\u001B[49m\u001B[43m(\u001B[49m\n\u001B[32m    173\u001B[39m \u001B[43m        \u001B[49m\u001B[43minputs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    174\u001B[39m \u001B[43m        \u001B[49m\u001B[43moutputs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    175\u001B[39m \u001B[43m        \u001B[49m\u001B[43mreturn_only_outputs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    176\u001B[39m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    177\u001B[39m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mBaseException\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m e:\n\u001B[32m    178\u001B[39m     run_manager.on_chain_error(e)\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/pet_projects/ai/langchain-course/.venv/lib/python3.12/site-packages/langchain_classic/chains/base.py:491\u001B[39m, in \u001B[36mChain.prep_outputs\u001B[39m\u001B[34m(self, inputs, outputs, return_only_outputs)\u001B[39m\n\u001B[32m    489\u001B[39m \u001B[38;5;28mself\u001B[39m._validate_outputs(outputs)\n\u001B[32m    490\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m.memory \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[32m--> \u001B[39m\u001B[32m491\u001B[39m     \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43mmemory\u001B[49m\u001B[43m.\u001B[49m\u001B[43msave_context\u001B[49m\u001B[43m(\u001B[49m\u001B[43minputs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43moutputs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    492\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m return_only_outputs:\n\u001B[32m    493\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m outputs\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/pet_projects/ai/langchain-course/.venv/lib/python3.12/site-packages/langchain_classic/memory/summary_buffer.py:101\u001B[39m, in \u001B[36mConversationSummaryBufferMemory.save_context\u001B[39m\u001B[34m(self, inputs, outputs)\u001B[39m\n\u001B[32m     99\u001B[39m \u001B[38;5;250m\u001B[39m\u001B[33;03m\"\"\"Save context from this conversation to buffer.\"\"\"\u001B[39;00m\n\u001B[32m    100\u001B[39m \u001B[38;5;28msuper\u001B[39m().save_context(inputs, outputs)\n\u001B[32m--> \u001B[39m\u001B[32m101\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43mprune\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/pet_projects/ai/langchain-course/.venv/lib/python3.12/site-packages/langchain_classic/memory/summary_buffer.py:115\u001B[39m, in \u001B[36mConversationSummaryBufferMemory.prune\u001B[39m\u001B[34m(self)\u001B[39m\n\u001B[32m    113\u001B[39m \u001B[38;5;250m\u001B[39m\u001B[33;03m\"\"\"Prune buffer if it exceeds max token limit.\"\"\"\u001B[39;00m\n\u001B[32m    114\u001B[39m buffer = \u001B[38;5;28mself\u001B[39m.chat_memory.messages\n\u001B[32m--> \u001B[39m\u001B[32m115\u001B[39m curr_buffer_length = \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43mllm\u001B[49m\u001B[43m.\u001B[49m\u001B[43mget_num_tokens_from_messages\u001B[49m\u001B[43m(\u001B[49m\u001B[43mbuffer\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    116\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m curr_buffer_length > \u001B[38;5;28mself\u001B[39m.max_token_limit:\n\u001B[32m    117\u001B[39m     pruned_memory = []\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/pet_projects/ai/langchain-course/.venv/lib/python3.12/site-packages/langchain_core/language_models/base.py:373\u001B[39m, in \u001B[36mBaseLanguageModel.get_num_tokens_from_messages\u001B[39m\u001B[34m(self, messages, tools)\u001B[39m\n\u001B[32m    368\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m tools \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[32m    369\u001B[39m     warnings.warn(\n\u001B[32m    370\u001B[39m         \u001B[33m\"\u001B[39m\u001B[33mCounting tokens in tool schemas is not yet supported. Ignoring tools.\u001B[39m\u001B[33m\"\u001B[39m,\n\u001B[32m    371\u001B[39m         stacklevel=\u001B[32m2\u001B[39m,\n\u001B[32m    372\u001B[39m     )\n\u001B[32m--> \u001B[39m\u001B[32m373\u001B[39m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43msum\u001B[39;49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43mget_num_tokens\u001B[49m\u001B[43m(\u001B[49m\u001B[43mget_buffer_string\u001B[49m\u001B[43m(\u001B[49m\u001B[43m[\u001B[49m\u001B[43mm\u001B[49m\u001B[43m]\u001B[49m\u001B[43m)\u001B[49m\u001B[43m)\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43;01mfor\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[43mm\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;129;43;01min\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[43mmessages\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/pet_projects/ai/langchain-course/.venv/lib/python3.12/site-packages/langchain_core/language_models/base.py:373\u001B[39m, in \u001B[36m<genexpr>\u001B[39m\u001B[34m(.0)\u001B[39m\n\u001B[32m    368\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m tools \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[32m    369\u001B[39m     warnings.warn(\n\u001B[32m    370\u001B[39m         \u001B[33m\"\u001B[39m\u001B[33mCounting tokens in tool schemas is not yet supported. Ignoring tools.\u001B[39m\u001B[33m\"\u001B[39m,\n\u001B[32m    371\u001B[39m         stacklevel=\u001B[32m2\u001B[39m,\n\u001B[32m    372\u001B[39m     )\n\u001B[32m--> \u001B[39m\u001B[32m373\u001B[39m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28msum\u001B[39m(\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43mget_num_tokens\u001B[49m\u001B[43m(\u001B[49m\u001B[43mget_buffer_string\u001B[49m\u001B[43m(\u001B[49m\u001B[43m[\u001B[49m\u001B[43mm\u001B[49m\u001B[43m]\u001B[49m\u001B[43m)\u001B[49m\u001B[43m)\u001B[49m \u001B[38;5;28;01mfor\u001B[39;00m m \u001B[38;5;129;01min\u001B[39;00m messages)\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/pet_projects/ai/langchain-course/.venv/lib/python3.12/site-packages/langchain_core/language_models/base.py:336\u001B[39m, in \u001B[36mBaseLanguageModel.get_num_tokens\u001B[39m\u001B[34m(self, text)\u001B[39m\n\u001B[32m    321\u001B[39m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34mget_num_tokens\u001B[39m(\u001B[38;5;28mself\u001B[39m, text: \u001B[38;5;28mstr\u001B[39m) -> \u001B[38;5;28mint\u001B[39m:\n\u001B[32m    322\u001B[39m \u001B[38;5;250m    \u001B[39m\u001B[33;03m\"\"\"Get the number of tokens present in the text.\u001B[39;00m\n\u001B[32m    323\u001B[39m \n\u001B[32m    324\u001B[39m \u001B[33;03m    Useful for checking if an input fits in a model's context window.\u001B[39;00m\n\u001B[32m   (...)\u001B[39m\u001B[32m    334\u001B[39m \n\u001B[32m    335\u001B[39m \u001B[33;03m    \"\"\"\u001B[39;00m\n\u001B[32m--> \u001B[39m\u001B[32m336\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mlen\u001B[39m(\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43mget_token_ids\u001B[49m\u001B[43m(\u001B[49m\u001B[43mtext\u001B[49m\u001B[43m)\u001B[49m)\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/pet_projects/ai/langchain-course/.venv/lib/python3.12/site-packages/langchain_core/language_models/base.py:319\u001B[39m, in \u001B[36mBaseLanguageModel.get_token_ids\u001B[39m\u001B[34m(self, text)\u001B[39m\n\u001B[32m    317\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m.custom_get_token_ids \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[32m    318\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m.custom_get_token_ids(text)\n\u001B[32m--> \u001B[39m\u001B[32m319\u001B[39m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43m_get_token_ids_default_method\u001B[49m\u001B[43m(\u001B[49m\u001B[43mtext\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/pet_projects/ai/langchain-course/.venv/lib/python3.12/site-packages/langchain_core/language_models/base.py:113\u001B[39m, in \u001B[36m_get_token_ids_default_method\u001B[39m\u001B[34m(text)\u001B[39m\n\u001B[32m    105\u001B[39m     warnings.warn(\n\u001B[32m    106\u001B[39m         \u001B[33m\"\u001B[39m\u001B[33mUsing fallback GPT-2 tokenizer for token counting. \u001B[39m\u001B[33m\"\u001B[39m\n\u001B[32m    107\u001B[39m         \u001B[33m\"\u001B[39m\u001B[33mToken counts may be inaccurate for non-GPT-2 models. \u001B[39m\u001B[33m\"\u001B[39m\n\u001B[32m    108\u001B[39m         \u001B[33m\"\u001B[39m\u001B[33mFor accurate counts, use a model-specific method if available.\u001B[39m\u001B[33m\"\u001B[39m,\n\u001B[32m    109\u001B[39m         stacklevel=\u001B[32m3\u001B[39m,\n\u001B[32m    110\u001B[39m     )\n\u001B[32m    111\u001B[39m     _GPT2_TOKENIZER_WARNED = \u001B[38;5;28;01mTrue\u001B[39;00m\n\u001B[32m--> \u001B[39m\u001B[32m113\u001B[39m tokenizer = \u001B[43mget_tokenizer\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    115\u001B[39m \u001B[38;5;66;03m# Pass verbose=False to suppress the \"Token indices sequence length is longer than\u001B[39;00m\n\u001B[32m    116\u001B[39m \u001B[38;5;66;03m# the specified maximum sequence length\" warning from HuggingFace. This warning is\u001B[39;00m\n\u001B[32m    117\u001B[39m \u001B[38;5;66;03m# about GPT-2's 1024 token context limit, but we're only using the tokenizer for\u001B[39;00m\n\u001B[32m    118\u001B[39m \u001B[38;5;66;03m# counting, not for model input.\u001B[39;00m\n\u001B[32m    119\u001B[39m \u001B[38;5;28;01mreturn\u001B[39;00m cast(\u001B[33m\"\u001B[39m\u001B[33mlist[int]\u001B[39m\u001B[33m\"\u001B[39m, tokenizer.encode(text, verbose=\u001B[38;5;28;01mFalse\u001B[39;00m))\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/pet_projects/ai/langchain-course/.venv/lib/python3.12/site-packages/langchain_core/language_models/base.py:93\u001B[39m, in \u001B[36mget_tokenizer\u001B[39m\u001B[34m()\u001B[39m\n\u001B[32m     87\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m _HAS_TRANSFORMERS:\n\u001B[32m     88\u001B[39m     msg = (\n\u001B[32m     89\u001B[39m         \u001B[33m\"\u001B[39m\u001B[33mCould not import transformers python package. \u001B[39m\u001B[33m\"\u001B[39m\n\u001B[32m     90\u001B[39m         \u001B[33m\"\u001B[39m\u001B[33mThis is needed in order to calculate get_token_ids. \u001B[39m\u001B[33m\"\u001B[39m\n\u001B[32m     91\u001B[39m         \u001B[33m\"\u001B[39m\u001B[33mPlease install it with `pip install transformers`.\u001B[39m\u001B[33m\"\u001B[39m\n\u001B[32m     92\u001B[39m     )\n\u001B[32m---> \u001B[39m\u001B[32m93\u001B[39m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mImportError\u001B[39;00m(msg)\n\u001B[32m     94\u001B[39m \u001B[38;5;66;03m# create a GPT-2 tokenizer instance\u001B[39;00m\n\u001B[32m     95\u001B[39m \u001B[38;5;28;01mreturn\u001B[39;00m GPT2TokenizerFast.from_pretrained(\u001B[33m\"\u001B[39m\u001B[33mgpt2\u001B[39m\u001B[33m\"\u001B[39m)\n",
      "\u001B[31mImportError\u001B[39m: Could not import transformers python package. This is needed in order to calculate get_token_ids. Please install it with `pip install transformers`."
     ]
    }
   ],
   "execution_count": 85
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-03-01T13:28:15.381256Z",
     "start_time": "2026-03-01T13:28:15.347188Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class ConversationSummaryBufferMessageHistory(BaseChatMessageHistory, BaseModel):\n",
    "    messages: list[BaseMessage] = Field(default_factory=list)\n",
    "    llm: ChatOllama = Field(default_factory=ChatOllama)\n",
    "    k: int = Field(default_factory=int)\n",
    "\n",
    "    def __init__(self, llm: ChatOllama, k: int):\n",
    "        super().__init__(llm=llm, k=k)\n",
    "\n",
    "    def add_messages(self, messages: list[BaseMessage]) -> None:\n",
    "        \"\"\"Add messages to the history, removing any messages beyond\n",
    "        the last `k` messages and summarizing the messages that we\n",
    "        drop.\n",
    "        \"\"\"\n",
    "        existing_summary: SystemMessage | None = None\n",
    "        old_messages: list[BaseMessage] | None = None\n",
    "        # see if we already have a summary message\n",
    "        if len(self.messages) > 0 and isinstance(self.messages[0], SystemMessage):\n",
    "            print(\">> Found existing summary\")\n",
    "            existing_summary = self.messages.pop(0)\n",
    "        # add the new messages to the history\n",
    "        self.messages.extend(messages)\n",
    "        # check if we have too many messages\n",
    "        if len(self.messages) > self.k:\n",
    "            print(\n",
    "                f\">> Found {len(self.messages)} messages, dropping \"\n",
    "                f\"oldest {len(self.messages) - self.k} messages.\")\n",
    "            # pull out the oldest messages...\n",
    "            old_messages = self.messages[:-self.k]\n",
    "            # ...and keep only the most recent messages\n",
    "            self.messages = self.messages[-self.k:]\n",
    "        if old_messages is None:\n",
    "            print(\">> No old messages to update summary with\")\n",
    "            # if we have no old_messages, we have nothing to update in summary\n",
    "            return\n",
    "        # construct the summary chat messages\n",
    "        summary_prompt = ChatPromptTemplate.from_messages([\n",
    "            SystemMessagePromptTemplate.from_template(\n",
    "                \"Given the existing conversation summary and the new messages, \"\n",
    "                \"generate a new summary of the conversation. Ensuring to maintain \"\n",
    "                \"as much relevant information as possible.\"\n",
    "            ),\n",
    "            HumanMessagePromptTemplate.from_template(\n",
    "                \"Existing conversation summary:\\n{existing_summary}\\n\\n\"\n",
    "                \"New messages:\\n{old_messages}\"\n",
    "            )\n",
    "        ])\n",
    "        # format the messages and invoke the LLM\n",
    "        new_summary = self.llm.invoke(\n",
    "            summary_prompt.format_messages(\n",
    "                existing_summary=existing_summary,\n",
    "                old_messages=old_messages\n",
    "            )\n",
    "        )\n",
    "        print(f\">> New summary: {new_summary.content}\")\n",
    "        # prepend the new summary to the history\n",
    "        self.messages = [SystemMessage(content=new_summary.content)] + self.messages\n",
    "\n",
    "    def clear(self) -> None:\n",
    "        \"\"\"Clear the history.\"\"\"\n",
    "        self.messages = []"
   ],
   "id": "e38a178aae53addf",
   "outputs": [],
   "execution_count": 87
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-03-01T13:36:03.134419Z",
     "start_time": "2026-03-01T13:36:03.116713Z"
    }
   },
   "cell_type": "code",
   "source": [
    "chat_map = {}\n",
    "def get_chat_history(session_id: str, llm: ChatOllama, k: int) -> ConversationSummaryBufferMessageHistory:\n",
    "    if session_id not in chat_map:\n",
    "        # if session ID doesn't exist, create a new chat history\n",
    "        chat_map[session_id] = ConversationSummaryBufferMessageHistory(llm=llm, k=k)\n",
    "    # return the chat history\n",
    "    return chat_map[session_id]"
   ],
   "id": "bd576ebdca96c785",
   "outputs": [],
   "execution_count": 88
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-03-01T13:37:15.178985Z",
     "start_time": "2026-03-01T13:37:15.165484Z"
    }
   },
   "cell_type": "code",
   "source": [
    "pipeline_with_history = RunnableWithMessageHistory(\n",
    "    pipeline,\n",
    "    get_session_history=get_chat_history,\n",
    "    input_messages_key=\"query\",\n",
    "    history_messages_key=\"history\",\n",
    "    history_factory_config=[\n",
    "        ConfigurableFieldSpec(\n",
    "            id=\"session_id\",\n",
    "            annotation=str,\n",
    "            name=\"Session ID\",\n",
    "            description=\"The session ID to use for the chat history\",\n",
    "            default=\"id_default\",\n",
    "        ),\n",
    "        ConfigurableFieldSpec(\n",
    "            id=\"llm\",\n",
    "            annotation=ChatOllama,\n",
    "            name=\"LLM\",\n",
    "            description=\"The LLM to use for the conversation summary\",\n",
    "            default=llm,\n",
    "        ),\n",
    "        ConfigurableFieldSpec(\n",
    "            id=\"k\",\n",
    "            annotation=int,\n",
    "            name=\"k\",\n",
    "            description=\"The number of messages to keep in the history\",\n",
    "            default=4,\n",
    "        )\n",
    "    ]\n",
    ")"
   ],
   "id": "d7a549c3083514d2",
   "outputs": [],
   "execution_count": 90
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-03-01T13:37:39.527138Z",
     "start_time": "2026-03-01T13:37:21.548528Z"
    }
   },
   "cell_type": "code",
   "source": [
    "pipeline_with_history.invoke(\n",
    "    {\"query\": \"Hi, my name is James\"},\n",
    "    config={\"session_id\": \"id_123\", \"llm\": llm, \"k\": 4}\n",
    ")\n",
    "chat_map[\"id_123\"].messages"
   ],
   "id": "f253b186d10a568a",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">> No old messages to update summary with\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[HumanMessage(content='Hi, my name is James', additional_kwargs={}, response_metadata={}),\n",
       " AIMessage(content=\"Hello, James! It's a pleasure to meet you. How are you doing today? I'd love to hear more about you or anything you'd like to discuss. ðŸ˜Š\", additional_kwargs={}, response_metadata={'model': 'qwen3:8b', 'created_at': '2026-03-01T13:37:39.42151Z', 'done': True, 'done_reason': 'stop', 'total_duration': 17824641875, 'load_duration': 3375739500, 'prompt_eval_count': 29, 'prompt_eval_duration': 1560252750, 'eval_count': 135, 'eval_duration': 12699428008, 'logprobs': None, 'model_name': 'qwen3:8b', 'model_provider': 'ollama'}, id='lc_run--019ca99e-2cba-7b50-988d-625eceaf5867-0', tool_calls=[], invalid_tool_calls=[], usage_metadata={'input_tokens': 29, 'output_tokens': 135, 'total_tokens': 164})]"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 91
  },
  {
   "metadata": {
    "ExecuteTime": {
     "start_time": "2026-03-01T13:37:58.857101Z"
    }
   },
   "cell_type": "code",
   "source": [
    "for i, msg in enumerate([\n",
    "    \"I'm researching the different types of conversational memory.\",\n",
    "    \"I have been looking at ConversationBufferMemory and ConversationBufferWindowMemory.\",\n",
    "    \"Buffer memory just stores the entire conversation\",\n",
    "    \"Buffer window memory stores the last k messages, dropping the rest.\"\n",
    "]):\n",
    "    print(f\"---\\nMessage {i+1}\\n---\\n\")\n",
    "    pipeline_with_history.invoke(\n",
    "        {\"query\": msg},\n",
    "        config={\"session_id\": \"id_123\", \"llm\": llm, \"k\": 4}\n",
    "    )"
   ],
   "id": "328fd2e87490a3df",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---\n",
      "Message 1\n",
      "---\n",
      "\n",
      ">> No old messages to update summary with\n",
      "---\n",
      "Message 2\n",
      "---\n",
      "\n"
     ]
    }
   ],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
